{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "alone-phase",
   "metadata": {},
   "source": [
    "\n",
    "## Biweekly report\n",
    "### Pragna\n",
    "### Mandadi\n",
    "### Comparative Analysis of CNN vs MCDNN and MCDNN vs MCDNN with Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-fisher",
   "metadata": {},
   "source": [
    "In this biweekly report, I would like to start off with the basic implementations of various neural networks and use these implementations to create a Multi-column Deep Neural Network inspired from the paper:https://arxiv.org/pdf/1202.2745.pdf Through this report I would like to get to know the implementation details of various neural nets which would help me with the future reports in making any tweaks and experimenting with these neural networks. \n",
    "I have also done a comparision analysis with CNN vs MCDNN and MCDNN vs MCDNN with dropout over limited MNIST datset due to constarints in time and resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-health",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.signal import pool\n",
    "from theano.tensor.nnet import conv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-egypt",
   "metadata": {},
   "source": [
    "For the implementation of various neural networks I chose to go with the theano library. I initally chose Theano because Theano attains high speed with problems involving high amounts data when there is GPU available. But unfortunately I couldnâ€™t get an access to a GPU and as I have already written most of my code I went along with it. but there are other benefits that came along.It knows how to take structures and convert them into very efficient code that uses numpy and some native libraries. It is mainly designed to handle the types of computation required for large neural network algorithms used in Deep Learning. Theano is a sort of hybrid between numpy and sympy, an attempt is made to combine the two into one powerful library. Some advantages of theano are as follows:  \n",
    "\n",
    "__Stability Optimization__: Theano can find out some unstable expressions and can use more stable means to evaluate them\n",
    "\n",
    "__Symbolic Differentiation__: Theano is smart enough to automatically create symbolic graphs for computing gradients\n",
    "That is why, it is a very popular library in the field of Deep Learning. \n",
    "This also gave me a an opportunity to make myself familiar with a new library. I have referred to the following links to learn Theano: \n",
    "https://www.geeksforgeeks.org/theano-in-python/\n",
    "http://ir.hit.edu.cn/~jguo/docs/notes/a_simple_tutorial_on_theano.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-executive",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "prepared-milan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/Theano/Theano/archive/master.zip\n",
      "  Using cached https://github.com/Theano/Theano/archive/master.zip\n",
      "Requirement already satisfied: numpy>=1.9.1 in ./opt/anaconda3/lib/python3.8/site-packages (from Theano==1.0.5+unknown) (1.19.2)\n",
      "Requirement already satisfied: scipy>=0.14 in ./opt/anaconda3/lib/python3.8/site-packages (from Theano==1.0.5+unknown) (1.6.1)\n",
      "Requirement already satisfied: six>=1.9.0 in ./opt/anaconda3/lib/python3.8/site-packages (from Theano==1.0.5+unknown) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade https://github.com/Theano/Theano/archive/master.zip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-return",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade https://github.com/Lasagne/Lasagne/archive/master.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-carolina",
   "metadata": {},
   "source": [
    "I have implemented a typical hidden layer of an MLP. The units are fully connected and I have used tanh as the activation function, taking the inspiration from the git hub repo(https://github.com/xanwerneck/ml_mnist) which implemented the MCDNN exactly as described in the paper I have mentioned earlier. \n",
    "W is initialized with W_values which is uniformly sampled from sqrt(-6./(n_in+n_hidden)) and sqrt(6./(n_in+n_hidden)) using a random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "black-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer(object):\n",
    "    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n",
    "                 activation=T.tanh):\n",
    "        self.input = input\n",
    "        if W is None:\n",
    "            W_values = numpy.asarray(\n",
    "                rng.uniform(\n",
    "                    low=-numpy.sqrt(6. / (n_in + n_out)),\n",
    "                    high=numpy.sqrt(6. / (n_in + n_out)),\n",
    "                    size=(n_in, n_out)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "        else:\n",
    "            W_values = numpy.asarray(W, dtype=theano.config.floatX)\n",
    "\n",
    "        if b is None:\n",
    "            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
    "        else:\n",
    "            b_values = numpy.asarray(b, dtype=theano.config.floatX)\n",
    "        self.W = theano.shared(value=W_values, name='W', borrow=True)\n",
    "        self.b = theano.shared(value=b_values, name='b', borrow=True)\n",
    "        lin_output = T.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if activation is None\n",
    "            else activation(lin_output)\n",
    "        )\n",
    "        self.params = [self.W, self.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-roberts",
   "metadata": {},
   "source": [
    "I have implemented the Dropout version of the hidden layer for comparative analysis later. Dropout is a technique for addressing the problem of overfitting by combining the predictions of many different large neural nets at test time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "chubby-credits",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutHiddenLayer(HiddenLayer):\n",
    "    def __init__(self, rng, input, n_in, n_out,\n",
    "                 activation, dropout_rate, W=None, b=None):\n",
    "        super(DropoutHiddenLayer, self).__init__(\n",
    "                rng=rng, input=input, n_in=n_in, n_out=n_out, W=W, b=b,\n",
    "                activation=activation)\n",
    "\n",
    "        self.output = _dropout_from_layer(rng, self.output, p=dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-tactics",
   "metadata": {},
   "source": [
    "A multilayer perceptron is a feedforward artificial neural network model that has one layer or more of hidden units & nonlinear activations. Intermediate layers usually have as activation function tanh or thesigmoid function (defined here by a HiddenLayer class)  while thetop layer is a softmax layer (defined here by a LogisticRegression class has a nonlinear activation function (usually tanh or sigmoid) . One can use many such hidden layers making the architecture deep. \n",
    "I have implemented MLP for the experimental purposes and I was also keen on implementing as many neural nets as possible in theano for me to gain expertise in both the theano library and the concepts of Neural Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "vertical-genealogy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
    "        self.hiddenLayer = HiddenLayer(\n",
    "            rng=rng,\n",
    "            input=input,\n",
    "            n_in=n_in,\n",
    "            n_out=n_hidden,\n",
    "            activation=T.tanh\n",
    "        )\n",
    "        self.logRegressionLayer = LogisticRegression(\n",
    "            input=self.hiddenLayer.output,\n",
    "            n_in=n_hidden,\n",
    "            n_out=n_out\n",
    "        )\n",
    "        self.L1 = (\n",
    "            abs(self.hiddenLayer.W).sum()\n",
    "            + abs(self.logRegressionLayer.W).sum()\n",
    "        )\n",
    "        self.L2_sqr = (\n",
    "            (self.hiddenLayer.W ** 2).sum()\n",
    "            + (self.logRegressionLayer.W ** 2).sum()\n",
    "        )\n",
    "        self.negative_log_likelihood = (\n",
    "            self.logRegressionLayer.negative_log_likelihood\n",
    "        )\n",
    "        self.errors = self.logRegressionLayer.errors\n",
    "        self.params = self.hiddenLayer.params + self.logRegressionLayer.params\n",
    "        self.input = input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-freeware",
   "metadata": {},
   "source": [
    "Logistic regression is a probabilistic, linear classifier. It is parametrized by a weight matrix :math:`W` and a bias vector :math:`b`. Classification is done by projecting data points onto a set of hyperplanes, the distance to which is used to determine a class membership probability.\n",
    "Mathematically, this can be written as:\n",
    ".. math::\n",
    "  P(Y=i|x, W,b) &= softmax_i(W x + b) \\\\\n",
    "                &= \\frac {e^{W_i x + b_i}} {\\sum_j e^{W_j x + b_j}}\n",
    "The output of the model or prediction is then done by taking the argmax of the vector whose i'th element is P(Y=i|x).\n",
    "The logistic regression is fully described by a weight matrix :math: W and bias vector :math:b. Classification is done by projecting data points onto a set of hyperplanes, the distance to which is used to determine a class membership probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "delayed-squad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import scipy.misc\n",
    "\n",
    "import pdb\n",
    "\n",
    "SS = 28\n",
    "class LogisticRegression(object):\n",
    "    def __init__(self, input, n_in, n_out, W=None, b=None):\n",
    "        if W is None:\n",
    "            W_values = numpy.zeros((n_in, n_out), dtype=theano.config.floatX)\n",
    "        else:\n",
    "            W_values = numpy.asarray(W, dtype=theano.config.floatX)\n",
    "\n",
    "        if b is None:\n",
    "            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
    "        else:\n",
    "            b_values = numpy.asarray(b, dtype=theano.config.floatX)\n",
    "\n",
    "        self.W = theano.shared(value=W_values, name='W', borrow=True)\n",
    "        self.b = theano.shared(value=b_values, name='b', borrow=True)\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-cholesterol",
   "metadata": {},
   "source": [
    "The below function negative_log_likelihoof return the mean of the negative log-likelihood of the prediction of this model under a given target distribution.\n",
    "The error function returns a float representing the number of errors in the minibatch over the total number of examples of the minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "passing-accident",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(LogisticRegression):\n",
    "    def negative_log_likelihood(self, y):\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "    def errors(self, y):\n",
    "        if y.ndim != self.y_pred.ndim:\n",
    "            raise TypeError(\n",
    "                'y should have the same shape as self.y_pred',\n",
    "                ('y', y.type, 'y_pred', self.y_pred.type)\n",
    "            )\n",
    "        if y.dtype.startswith('int'):\n",
    "            return T.mean(T.neq(self.y_pred, y))\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-raising",
   "metadata": {},
   "source": [
    "ConvOp is the main workhorse for implementing a convolutional layer in Theano. ConvOp is used by theano.tensor.signal.conv2d, which takes two symbolic inputs:\n",
    "    a 4D tensor corresponding to a mini-batch of input images. The shape of the tensor is as follows: [mini-batch size, number of input feature maps, image height, image width].\n",
    "    a 4D tensor corresponding to the weight matrix W. The shape of the tensor is: [number of feature maps at layer m, number of feature maps at layer m-1, filter height, filter width]\n",
    "nnet.conv2d: This is the standard operator for convolutional neural networks working with batches of multi-channel 2D images\n",
    "Another important concept of CNNs is max-pooling, which is a form of non-linear down-sampling. Max-pooling partitions the input image into a set of non-overlapping rectangles and, for each such sub-region, outputs the maximum value.\n",
    "Max-pooling is useful in vision for two reasons:\n",
    "    By eliminating non-maximal values, it reduces computation for upper layers.\n",
    "    It provides a form of translation invariance. Imagine cascading a max-pooling layer with a convolutional layer. There are 8 directions in which one can translate the input image by a single pixel. If max-pooling is done over a 2x2 region, 3 out of these 8 possible configurations will produce exactly the same output at the convolutional layer. For max-pooling over a 3x3 window, this jumps to 5/8.\n",
    "\n",
    "Max-pooling is done in Theano by way of theano.tensor.signal.downsample.max_pool_2d. This function takes as input an N dimensional tensor (where N >= 2) and a downscaling factor and performs max-pooling over the 2 trailing dimensions of the tensor.\n",
    "\n",
    "LeNetConvPoolLayer class, implements a {convolution + max-pooling} layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fancy-state",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNetConvPoolLayer(object):\n",
    "    def __init__(self, rng, input, filter_shape, image_shape, poolsize=(2, 2), W=None, b=None):\n",
    "        assert image_shape[1] == filter_shape[1]\n",
    "        self.input = input\n",
    "        fan_in = numpy.prod(filter_shape[1:])\n",
    "        fan_out = (filter_shape[0] * numpy.prod(filter_shape[2:]) /\n",
    "                   numpy.prod(poolsize))\n",
    "        if W is None:\n",
    "            W_bound = numpy.sqrt(6. / (fan_in + fan_out))\n",
    "            W_values = numpy.asarray(\n",
    "                rng.uniform(low=-W_bound, high=W_bound, size=filter_shape),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "        else:\n",
    "            W_values = W\n",
    "\n",
    "        if b is None:\n",
    "            b_values = numpy.zeros((filter_shape[0],), dtype=theano.config.floatX)\n",
    "        else:\n",
    "            b_values = b\n",
    "\n",
    "        self.W = theano.shared(value=W_values, name='W', borrow=True)\n",
    "        self.b = theano.shared(value=b_values, name='b', borrow=True)\n",
    "\n",
    "        conv_out = conv.conv2d(\n",
    "            input=input,\n",
    "            filters=self.W,\n",
    "            filter_shape=filter_shape,\n",
    "            image_shape=image_shape\n",
    "        )\n",
    "\n",
    "        if (poolsize[0] == 1 and poolsize[1] == 1):\n",
    "            pooled_out = conv_out\n",
    "        else:\n",
    "            pooled_out = pool.pool_2d(\n",
    "                input=conv_out,\n",
    "                ds=poolsize,\n",
    "                ignore_border=True\n",
    "            )\n",
    "\n",
    "        self.output = T.tanh(pooled_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n",
    "        self.params = [self.W, self.b]\n",
    "        self.input = input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-fault",
   "metadata": {},
   "source": [
    "Below is the dropout version of LeNetConvPoolLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "municipal-combination",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutLeNetConvPoolLayer(LeNetConvPoolLayer):\n",
    "    def __init__(self, rng, input, filter_shape, image_shape, poolsize,\n",
    "               dropout_rate, W=None, b=None):\n",
    "        super(DropoutLeNetConvPoolLayer, self).__init__(\n",
    "          rng=rng, input=input, filter_shape=filter_shape, image_shape=image_shape,\n",
    "          poolsize=poolsize, W=W, b=b)\n",
    "\n",
    "        self.output = _dropout_from_layer(rng, self.output, p=dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-husband",
   "metadata": {},
   "source": [
    "This function takes a layer (which can be either a layer of units in an MLP or a layer of feature maps in a CNN) and drop units from the layer with a probability of p (or in the case of CNN pixels from feature maps with a probability of p)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "middle-fellowship",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dropout_from_layer(rng, layer, p):\n",
    "    \"\"\"p is the probablity of dropping a unit\n",
    "    \"\"\"\n",
    "    srng = theano.tensor.shared_randomstreams.RandomStreams(\n",
    "            rng.randint(999999))\n",
    "    # p=1-p because 1's indicate keep and p is probability of dropping\n",
    "    mask = srng.binomial(n=1, p=1-p, size=layer.shape)\n",
    "    # The cast is important because\n",
    "    # int * float32 = float64 which pulls things off the gpu\n",
    "    output = layer * T.cast(mask, theano.config.floatX)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-copper",
   "metadata": {},
   "source": [
    "For my experiments I have used the MNIST datset. The load dataset function loads the MNIST dataset. The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning. You can download the datasey from in the pickeled format: http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz\n",
    "Once the dataset is loaded I have reduced the size of training to 10000 which is usually 50000 and reduced the validation set to 2000 which is usally 10000 and the test set to 2000 as well which again was 10000. I tried to preserve the ratio of training to validation and to testing even though I reduced the entire dataset due to the lack of computational effeciency and resources.\n",
    "Once the train, test and validation sets are determined we need to process the data based the normalization width of the particular DNN column that we are trying to run this data on. For that we use prepare_digits and prepare_digits inturn calls the normalize_digit function which helps in resizing the images which are usually 28x28 and add any padding if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "recent-bridal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_digits(sets, end_size, digit_normalized_width):\n",
    "        set_x, set_y = sets[0], sets[1]\n",
    "        out = numpy.ndarray((set_x.shape[0], end_size**2), dtype=numpy.float32)\n",
    "\n",
    "        for i in range(0,set_x.shape[0]):\n",
    "            x = set_x[i].reshape((SS,SS))\n",
    "            if digit_normalized_width and (set_y[i] - 1): # don't normalize images of digit '1'\n",
    "                out[i] = normalize_digit(x, digit_normalized_width, end_size).reshape(end_size**2)\n",
    "            else:\n",
    "                out[i] = pad_image(x, end_size).reshape(end_size**2)\n",
    "        return out\n",
    "\n",
    "def pad_image(x, end_size):\n",
    "        cs = x.shape[0]\n",
    "        padding = end_size - cs\n",
    "        bp = round(padding / 2)\n",
    "        ap = padding - bp\n",
    "        pads = (bp,ap)\n",
    "        if bp + ap > 0:\n",
    "            return numpy.pad(x,(pads,pads),mode='constant').reshape(end_size**2)\n",
    "        else:\n",
    "            si = -bp\n",
    "            ei = cs + ap\n",
    "            return x[si:ei, si:ei].reshape(end_size**2)\n",
    "from PIL import Image\n",
    "def normalize_digit(x, digit_normalized_width, end_size):\n",
    "        width_diff = digit_normalized_width - sum(sum(x) != 0)\n",
    "        if width_diff:\n",
    "            nd = SS + width_diff # new dim\n",
    "            new_size = nd, nd\n",
    "            im = Image.fromarray(x)\n",
    "            normalized_image = im.resize(new_size, Image.ANTIALIAS)\n",
    "            x = numpy.array(normalized_image.getdata(), dtype=numpy.float32).reshape((nd,nd)) / 255\n",
    "        return pad_image(x, end_size)\n",
    "    \n",
    "def subtract_channel_mean(dataset, image_shape, channel_means, accuracy_dtype):\n",
    "        orig_shape = dataset[0].shape\n",
    "        full_shape = (dataset[0].shape[0], image_shape[0], image_shape[1], image_shape[2])\n",
    "        xs = numpy.asarray(dataset[0].reshape(full_shape), dtype=accuracy_dtype)\n",
    "        xs[:,:,:,0] -= channel_means[0]\n",
    "        xs[:,:,:,1] -= channel_means[1]\n",
    "        xs[:,:,:,2] -= channel_means[2]\n",
    "        return (xs.reshape(orig_shape), dataset[1])\n",
    "\n",
    "def divide_channel_max(dataset, image_shape, channel_maxes):\n",
    "        orig_shape = dataset[0].shape\n",
    "        full_shape = (dataset[0].shape[0], image_shape[0], image_shape[1], image_shape[2])\n",
    "        xs = numpy.asarray(dataset[0].reshape(full_shape), dtype='float32')\n",
    "        xs[:,:,:,0] /= channel_maxes[0]\n",
    "        xs[:,:,:,1] /= channel_maxes[1]\n",
    "        xs[:,:,:,2] /= channel_maxes[2]\n",
    "        return (xs.reshape(orig_shape), dataset[1])\n",
    "\n",
    "def load_data(dataset, digit_normalized_width=0, digit_out_image_size=SS,\n",
    "              conserve_gpu_memory=False, center=0, normalize=0, image_shape=None, y_values_only=False):\n",
    "    data_dir, data_file = os.path.split(dataset)\n",
    "    data_ext = '.'.join(data_file.split('.')[1:])\n",
    "    input_pixel_max = 1 if data_file == 'mnist.pkl.gz' else 255\n",
    "\n",
    "    if data_file == 'mnist.pkl.gz':\n",
    "        if data_dir == \"\" and not os.path.isfile(dataset):\n",
    "            new_path = os.path.join(\n",
    "                os.path.split(os.path.abspath(''))[0],\n",
    "                \"data\",\n",
    "                dataset\n",
    "            )\n",
    "            if os.path.isfile(new_path) or data_file == 'mnist.pkl.gz':\n",
    "                dataset = new_path\n",
    "\n",
    "        if (not os.path.isfile(dataset)) and data_file == 'mnist.pkl.gz':\n",
    "            import urllib.request\n",
    "            origin = (\n",
    "                'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
    "            )\n",
    "            print('Downloading data from %s' % origin)\n",
    "            print(data_dir)\n",
    "            urllib.request.urlretrieve(origin, dataset)\n",
    "\n",
    "    print('... loading data')\n",
    "    if data_file == 'mnist.pkl.gz':\n",
    "        f = gzip.open(dataset, 'rb')\n",
    "        u = pickle._Unpickler(f)\n",
    "        u.encoding = 'latin1'\n",
    "        train_set, valid_set, test_set = u.load()\n",
    "        train_set = (train_set[0][:10000],train_set[1][:10000])\n",
    "        valid_set = (valid_set[0][:2000],valid_set[1][:2000])\n",
    "        test_set = (test_set[0][:2000],test_set[1][:2000])\n",
    "        if digit_normalized_width or (digit_out_image_size != SS):\n",
    "            if digit_normalized_width:\n",
    "                print('... normalizing digits to width %i with extra padding %i' % (digit_normalized_width, digit_out_image_size - SS))\n",
    "            else:\n",
    "                print('... (un)padding digits from %i -> %i' % (SS, digit_out_image_size))\n",
    "            train_set = (prepare_digits(train_set, digit_out_image_size, digit_normalized_width), train_set[1])\n",
    "            valid_set = (prepare_digits(valid_set, digit_out_image_size, digit_normalized_width), valid_set[1])\n",
    "            test_set =  (prepare_digits(test_set, digit_out_image_size, digit_normalized_width),  test_set[1])\n",
    "        else:\n",
    "            print('... skipping digit normalization and image padding')\n",
    "\n",
    "        f.close()\n",
    "    elif data_ext == 'npz':\n",
    "        with numpy.load(dataset) as archive:\n",
    "            train_set = (archive['arr_0'], archive['arr_1'])\n",
    "            valid_set = (archive['arr_2'], archive['arr_3'])\n",
    "            test_set =  (archive['arr_4'], archive['arr_5'])\n",
    "    else:\n",
    "        raise ValueError(\"unsupported data extension %s\" % data_ext)\n",
    "\n",
    "    if y_values_only:\n",
    "        print('... returning y values')\n",
    "        return (train_set[1], valid_set[1], test_set[1])\n",
    "\n",
    "    accuracy_dtype = int\n",
    "    if center == 1:\n",
    "        assert(image_shape)\n",
    "        print('... subtracting channel means')\n",
    "        channel_means = numpy.mean(train_set[0].reshape(train_set[0].shape[0], *image_shape), axis=(0,1,2))\n",
    "        train_set = subtract_channel_mean(train_set, image_shape, channel_means, accuracy_dtype)\n",
    "        valid_set = subtract_channel_mean(valid_set, image_shape, channel_means, accuracy_dtype)\n",
    "        test_set = subtract_channel_mean(test_set, image_shape, channel_means, accuracy_dtype)\n",
    "    elif center == 2:\n",
    "        print('... subtracting mean images')\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    if not input_pixel_max == 1:\n",
    "        if normalize == 1:\n",
    "            print('... normalizing with max channel pixel values')\n",
    "            channel_maxes = numpy.array(255 - channel_means, dtype=accuracy_dtype)\n",
    "            train_set = divide_channel_max(train_set, image_shape, channel_maxes)\n",
    "            valid_set = divide_channel_max(valid_set, image_shape, channel_maxes)\n",
    "            test_set = divide_channel_max(test_set, image_shape, channel_maxes)\n",
    "        elif normalize == 2:\n",
    "            print('... normalizing with image std deviations')\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    print('... sharing data')\n",
    "    def share_dataset(data_xy, borrow=True, image_shape=None, conserve_gpu_memory=False):\n",
    "        data_x, data_y = data_xy\n",
    "        if image_shape:\n",
    "            data_x = data_x.reshape(data_x.shape[0], *image_shape)\n",
    "            data_x = numpy.rollaxis(data_x, 3, 1)\n",
    "\n",
    "        if conserve_gpu_memory:\n",
    "            shared_x = theano.tensor._shared(numpy.asarray(data_x,\n",
    "                                               dtype=theano.config.floatX),\n",
    "                                 borrow=borrow)\n",
    "            shared_y = theano.tensor._shared(numpy.asarray(data_y,\n",
    "                                               dtype=theano.config.floatX),\n",
    "                                 borrow=borrow)\n",
    "        else:\n",
    "            shared_x = theano.shared(numpy.asarray(data_x,\n",
    "                                                   dtype=theano.config.floatX),\n",
    "                                     borrow=borrow)\n",
    "            shared_y = theano.shared(numpy.asarray(data_y,\n",
    "                                                   dtype=theano.config.floatX),\n",
    "                                     borrow=borrow)\n",
    "        return shared_x, T.cast(shared_y, 'int32')\n",
    "\n",
    "    test_set_x, test_set_y = share_dataset(test_set, image_shape=image_shape, conserve_gpu_memory=conserve_gpu_memory)\n",
    "    valid_set_x, valid_set_y = share_dataset(valid_set, image_shape=image_shape, conserve_gpu_memory=conserve_gpu_memory)\n",
    "    train_set_x, train_set_y = share_dataset(train_set, image_shape=image_shape, conserve_gpu_memory=conserve_gpu_memory)\n",
    "\n",
    "    rval = [(train_set_x, train_set_y), (valid_set_x, valid_set_y),\n",
    "            (test_set_x, test_set_y)]\n",
    "    return rval\n",
    "\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "informative-norman",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lenet5(learning_rate=0.1, n_epochs=50,\n",
    "                    dataset='mnist.pkl.gz',\n",
    "                    nkerns=[20, 50], batch_size=100):\n",
    "\n",
    "    rng = numpy.random.RandomState(23455)\n",
    "\n",
    "    datasets = load_data(dataset)\n",
    "\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    valid_set_x, valid_set_y = datasets[1]\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0]\n",
    "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0]\n",
    "    n_test_batches = test_set_x.get_value(borrow=True).shape[0]\n",
    "    n_train_batches //= batch_size\n",
    "    n_valid_batches //= batch_size\n",
    "    n_test_batches //= batch_size\n",
    "    index = T.lscalar()\n",
    "    x = T.matrix('x')   \n",
    "    print('... building the model')\n",
    "\n",
    "    layer0_input = x.reshape((batch_size, 1, 28, 28))\n",
    "\n",
    "    layer0 = LeNetConvPoolLayer(\n",
    "        rng,\n",
    "        input=layer0_input,\n",
    "        image_shape=(batch_size, 1, 28, 28),\n",
    "        filter_shape=(nkerns[0], 1, 5, 5),\n",
    "        poolsize=(2, 2)\n",
    "    )\n",
    "\n",
    "    layer1 = LeNetConvPoolLayer(\n",
    "        rng,\n",
    "        input=layer0.output,\n",
    "        image_shape=(batch_size, nkerns[0], 12, 12),\n",
    "        filter_shape=(nkerns[1], nkerns[0], 5, 5),\n",
    "        poolsize=(2, 2)\n",
    "    )\n",
    "\n",
    "    layer2_input = layer1.output.flatten(2)\n",
    "\n",
    "    layer2 = HiddenLayer(\n",
    "        rng,\n",
    "        input=layer2_input,\n",
    "        n_in=nkerns[1] * 4 * 4,\n",
    "        n_out=500,\n",
    "        activation=T.tanh\n",
    "    )\n",
    "    \n",
    "    layer3 = LogisticRegression(input=layer2.output, n_in=500, n_out=10)\n",
    "\n",
    "    cost = layer3.negative_log_likelihood(y)\n",
    "\n",
    "    test_model = theano.function(\n",
    "        [index],\n",
    "        layer3.errors(y),\n",
    "        givens={\n",
    "            x: test_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: test_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    validate_model = theano.function(\n",
    "        [index],\n",
    "        layer3.errors(y),\n",
    "        givens={\n",
    "            x: valid_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: valid_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    params = layer3.params + layer2.params + layer1.params + layer0.params\n",
    "\n",
    "    grads = T.grad(cost, params)\n",
    "\n",
    "    updates = [\n",
    "        (param_i, param_i - learning_rate * grad_i)\n",
    "        for param_i, grad_i in zip(params, grads)\n",
    "    ]\n",
    "\n",
    "    train_model = theano.function(\n",
    "        [index],\n",
    "        cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "    print('... training')\n",
    "    patience = 10000\n",
    "    patience_increase = 2 \n",
    "    improvement_threshold = 0.995 \n",
    "    validation_frequency = min(n_train_batches, patience / 2)\n",
    "                \n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    best_iter = 0\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    epoch = 0\n",
    "    done_looping = False\n",
    "\n",
    "    while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "            if iter % 100 == 0:\n",
    "                print('training @ iter = ', iter)\n",
    "            cost_ij = train_model(minibatch_index)\n",
    "\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "\n",
    "                # compute zero-one loss on validation set\n",
    "                validation_losses = [validate_model(i) for i\n",
    "                                     in range(n_valid_batches)]\n",
    "                this_validation_loss = numpy.mean(validation_losses)\n",
    "                print('epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                      (epoch, minibatch_index + 1, n_train_batches,\n",
    "                       this_validation_loss * 100.))\n",
    "\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "\n",
    "                    #improve patience if loss improvement is good enough\n",
    "                    if this_validation_loss < best_validation_loss *  \\\n",
    "                       improvement_threshold:\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    # save best validation score and iteration number\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    best_iter = iter\n",
    "\n",
    "                    # test it on the test set\n",
    "                    test_losses = [\n",
    "                        test_model(i)\n",
    "                        for i in range(n_test_batches)\n",
    "                    ]\n",
    "                    test_score = numpy.mean(test_losses)\n",
    "                    print(('     epoch %i, minibatch %i/%i, test error of '\n",
    "                           'best model %f %%') %\n",
    "                          (epoch, minibatch_index + 1, n_train_batches,\n",
    "                           test_score * 100.))\n",
    "\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    print('Optimization complete.')\n",
    "    print('Best validation score of %f %% obtained at iteration %i, '\n",
    "          'with test performance %f %%' %\n",
    "          (best_validation_loss * 100., best_iter + 1, test_score * 100.))\n",
    "    print (sys.stderr, ('The code for file ' +\n",
    "                          os.path.split(os.path.abspath(''))[1] +\n",
    "                          ' ran for %.2fm' % ((end_time - start_time) / 60.)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-district",
   "metadata": {},
   "source": [
    "Below is the demonstration of training the MNIST dataset on the CNN (LeNetConvPoolLayer) with a batch size of 100 and over 50 epochs and. We can see in the results that the best validtion score is 1.8% which was converging towards the end of the epochs and the test performance on the trained model was 2.65%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "intense-benchmark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading data\n",
      "... skipping digit normalization and image padding\n",
      "... sharing data\n",
      "... building the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-68-5cba53fdea8d>:39: UserWarning: DEPRECATION: the 'ds' parameter is not going to exist anymore as it is going to be replaced by the parameter 'ws'.\n",
      "  pooled_out = pool.pool_2d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... training\n",
      "training @ iter =  0\n",
      "epoch 1, minibatch 100/100, validation error 12.450000 %\n",
      "     epoch 1, minibatch 100/100, test error of best model 13.650000 %\n",
      "training @ iter =  100\n",
      "epoch 2, minibatch 100/100, validation error 8.550000 %\n",
      "     epoch 2, minibatch 100/100, test error of best model 10.100000 %\n",
      "training @ iter =  200\n",
      "epoch 3, minibatch 100/100, validation error 6.500000 %\n",
      "     epoch 3, minibatch 100/100, test error of best model 8.050000 %\n",
      "training @ iter =  300\n",
      "epoch 4, minibatch 100/100, validation error 5.000000 %\n",
      "     epoch 4, minibatch 100/100, test error of best model 6.450000 %\n",
      "training @ iter =  400\n",
      "epoch 5, minibatch 100/100, validation error 4.300000 %\n",
      "     epoch 5, minibatch 100/100, test error of best model 5.800000 %\n",
      "training @ iter =  500\n",
      "epoch 6, minibatch 100/100, validation error 3.700000 %\n",
      "     epoch 6, minibatch 100/100, test error of best model 4.850000 %\n",
      "training @ iter =  600\n",
      "epoch 7, minibatch 100/100, validation error 3.450000 %\n",
      "     epoch 7, minibatch 100/100, test error of best model 4.500000 %\n",
      "training @ iter =  700\n",
      "epoch 8, minibatch 100/100, validation error 3.300000 %\n",
      "     epoch 8, minibatch 100/100, test error of best model 4.100000 %\n",
      "training @ iter =  800\n",
      "epoch 9, minibatch 100/100, validation error 3.050000 %\n",
      "     epoch 9, minibatch 100/100, test error of best model 3.950000 %\n",
      "training @ iter =  900\n",
      "epoch 10, minibatch 100/100, validation error 2.950000 %\n",
      "     epoch 10, minibatch 100/100, test error of best model 3.750000 %\n",
      "training @ iter =  1000\n",
      "epoch 11, minibatch 100/100, validation error 2.950000 %\n",
      "training @ iter =  1100\n",
      "epoch 12, minibatch 100/100, validation error 2.750000 %\n",
      "     epoch 12, minibatch 100/100, test error of best model 3.700000 %\n",
      "training @ iter =  1200\n",
      "epoch 13, minibatch 100/100, validation error 2.550000 %\n",
      "     epoch 13, minibatch 100/100, test error of best model 3.550000 %\n",
      "training @ iter =  1300\n",
      "epoch 14, minibatch 100/100, validation error 2.400000 %\n",
      "     epoch 14, minibatch 100/100, test error of best model 3.300000 %\n",
      "training @ iter =  1400\n",
      "epoch 15, minibatch 100/100, validation error 2.300000 %\n",
      "     epoch 15, minibatch 100/100, test error of best model 3.250000 %\n",
      "training @ iter =  1500\n",
      "epoch 16, minibatch 100/100, validation error 2.300000 %\n",
      "training @ iter =  1600\n",
      "epoch 17, minibatch 100/100, validation error 2.300000 %\n",
      "training @ iter =  1700\n",
      "epoch 18, minibatch 100/100, validation error 2.250000 %\n",
      "     epoch 18, minibatch 100/100, test error of best model 3.150000 %\n",
      "training @ iter =  1800\n",
      "epoch 19, minibatch 100/100, validation error 2.150000 %\n",
      "     epoch 19, minibatch 100/100, test error of best model 3.100000 %\n",
      "training @ iter =  1900\n",
      "epoch 20, minibatch 100/100, validation error 2.150000 %\n",
      "training @ iter =  2000\n",
      "epoch 21, minibatch 100/100, validation error 2.100000 %\n",
      "     epoch 21, minibatch 100/100, test error of best model 3.100000 %\n",
      "training @ iter =  2100\n",
      "epoch 22, minibatch 100/100, validation error 2.100000 %\n",
      "training @ iter =  2200\n",
      "epoch 23, minibatch 100/100, validation error 2.100000 %\n",
      "training @ iter =  2300\n",
      "epoch 24, minibatch 100/100, validation error 2.000000 %\n",
      "     epoch 24, minibatch 100/100, test error of best model 2.800000 %\n",
      "training @ iter =  2400\n",
      "epoch 25, minibatch 100/100, validation error 2.000000 %\n",
      "training @ iter =  2500\n",
      "epoch 26, minibatch 100/100, validation error 2.000000 %\n",
      "training @ iter =  2600\n",
      "epoch 27, minibatch 100/100, validation error 2.000000 %\n",
      "training @ iter =  2700\n",
      "epoch 28, minibatch 100/100, validation error 1.950000 %\n",
      "     epoch 28, minibatch 100/100, test error of best model 2.800000 %\n",
      "training @ iter =  2800\n",
      "epoch 29, minibatch 100/100, validation error 1.950000 %\n",
      "training @ iter =  2900\n",
      "epoch 30, minibatch 100/100, validation error 1.950000 %\n",
      "training @ iter =  3000\n",
      "epoch 31, minibatch 100/100, validation error 1.950000 %\n",
      "training @ iter =  3100\n",
      "epoch 32, minibatch 100/100, validation error 1.950000 %\n",
      "training @ iter =  3200\n",
      "epoch 33, minibatch 100/100, validation error 1.950000 %\n",
      "training @ iter =  3300\n",
      "epoch 34, minibatch 100/100, validation error 1.950000 %\n",
      "training @ iter =  3400\n",
      "epoch 35, minibatch 100/100, validation error 1.950000 %\n",
      "training @ iter =  3500\n",
      "epoch 36, minibatch 100/100, validation error 1.950000 %\n",
      "training @ iter =  3600\n",
      "epoch 37, minibatch 100/100, validation error 1.950000 %\n",
      "training @ iter =  3700\n",
      "epoch 38, minibatch 100/100, validation error 1.950000 %\n",
      "training @ iter =  3800\n",
      "epoch 39, minibatch 100/100, validation error 1.950000 %\n",
      "training @ iter =  3900\n",
      "epoch 40, minibatch 100/100, validation error 1.950000 %\n",
      "training @ iter =  4000\n",
      "epoch 41, minibatch 100/100, validation error 1.900000 %\n",
      "     epoch 41, minibatch 100/100, test error of best model 2.700000 %\n",
      "training @ iter =  4100\n",
      "epoch 42, minibatch 100/100, validation error 1.900000 %\n",
      "training @ iter =  4200\n",
      "epoch 43, minibatch 100/100, validation error 1.850000 %\n",
      "     epoch 43, minibatch 100/100, test error of best model 2.650000 %\n",
      "training @ iter =  4300\n",
      "epoch 44, minibatch 100/100, validation error 1.800000 %\n",
      "     epoch 44, minibatch 100/100, test error of best model 2.650000 %\n",
      "training @ iter =  4400\n",
      "epoch 45, minibatch 100/100, validation error 1.800000 %\n",
      "training @ iter =  4500\n",
      "epoch 46, minibatch 100/100, validation error 1.800000 %\n",
      "training @ iter =  4600\n",
      "epoch 47, minibatch 100/100, validation error 1.800000 %\n",
      "training @ iter =  4700\n",
      "epoch 48, minibatch 100/100, validation error 1.800000 %\n",
      "training @ iter =  4800\n",
      "epoch 49, minibatch 100/100, validation error 1.800000 %\n",
      "training @ iter =  4900\n",
      "epoch 50, minibatch 100/100, validation error 1.800000 %\n",
      "Optimization complete.\n",
      "Best validation score of 1.800000 % obtained at iteration 4400, with test performance 2.650000 %\n",
      "<ipykernel.iostream.OutStream object at 0x7ffed0646790> The code for file pragnamandadi ran for 16.14m\n"
     ]
    }
   ],
   "source": [
    "evaluate_lenet5()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-grenada",
   "metadata": {},
   "source": [
    "Below is the implementation of one DNN column in MCDNN. Each DNN column is initialized with the normalized width and the extent of distortion that needs to be done to the images in data. I have implemented the DNN class both with dropout and without dropout. The dropout rates I have taken are 20% for the input units and 50% for the hidden units.\n",
    "Each DNN column starts with Convolution layers with max-pooling to reduce the size of the images and create feature maps. After the convolution layer a hidden layer is introduced to transform the output into something the Logistic regrssion layer can use which inturn gives the probabilities of each class a particular image can belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "refined-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    " class DNNColumn(object):\n",
    "\n",
    "    def __init__(self, ds=None, nkerns=[32, 48], batch_size=100, normalized_width=0, distortion=0,\n",
    "                    params=[None, None,None, None,None, None,None, None],dropout=False):\n",
    "        layer3_W, layer3_b, layer2_W, layer2_b, layer1_W, layer1_b, layer0_W, layer0_b = params    \n",
    "        train_set_x, train_set_y = ds[0]\n",
    "        valid_set_x, valid_set_y = ds[1]\n",
    "        test_set_x, test_set_y   = ds[2]\n",
    "        dropout_rates = [0.2, 0.2, 0.2, 0.5]\n",
    "        self.n_train_batches  = train_set_x.get_value(borrow=True).shape[0]\n",
    "        self.n_valid_batches  = valid_set_x.get_value(borrow=True).shape[0]\n",
    "        self.n_test_batches   = test_set_x.get_value(borrow=True).shape[0]\n",
    "        self.n_train_batches //= batch_size\n",
    "        self.n_valid_batches //= batch_size\n",
    "        self.n_test_batches  //= batch_size\n",
    "        index = T.lscalar()\n",
    "        learning_rate = T.fscalar()\n",
    "        rng = numpy.random.RandomState(23455)\n",
    "        print('... building the dnn column')\n",
    "        x = T.matrix('x')\n",
    "        y = T.ivector('y')\n",
    "        layer0_input = x.reshape((batch_size, 1, 29, 29))\n",
    "        if dropout:\n",
    "            layer0 = DropoutLeNetConvPoolLayer(\n",
    "            rng,\n",
    "            input=layer0_input,\n",
    "            image_shape=(batch_size, 1, 29, 29),\n",
    "            filter_shape=(nkerns[0], 1, 4, 4),\n",
    "            poolsize=(2, 2),\n",
    "            dropout_rate= dropout_rates[1]\n",
    "        )\n",
    "            layer1 = DropoutLeNetConvPoolLayer(\n",
    "                rng,\n",
    "                input=layer0.output,\n",
    "                image_shape=(batch_size, nkerns[0], 13, 13),\n",
    "                filter_shape=(nkerns[1], nkerns[0], 5, 5),\n",
    "                poolsize=(3, 3),\n",
    "                dropout_rate= dropout_rates[2]\n",
    "            )\n",
    "        else:\n",
    "            layer0 = LeNetConvPoolLayer(\n",
    "                rng,\n",
    "                input=layer0_input,\n",
    "                image_shape=(batch_size, 1, 29, 29),\n",
    "                filter_shape=(nkerns[0], 1, 4, 4),\n",
    "                poolsize=(2, 2),\n",
    "                W=layer0_W,\n",
    "                b=layer0_b\n",
    "            )\n",
    "            layer1 = LeNetConvPoolLayer(\n",
    "                rng,\n",
    "                input=layer0.output,\n",
    "                image_shape=(batch_size, nkerns[0], 13, 13),\n",
    "                filter_shape=(nkerns[1], nkerns[0], 5, 5),\n",
    "                poolsize=(3, 3),\n",
    "                W=layer1_W,\n",
    "                b=layer1_b\n",
    "            )\n",
    "\n",
    "        layer2_input = layer1.output.flatten(2)\n",
    "        if dropout:\n",
    "            layer2 = DropoutHiddenLayer(\n",
    "            rng,\n",
    "            input=layer2_input,\n",
    "            n_in=nkerns[1] * 3 * 3,\n",
    "            n_out=150,\n",
    "            dropout_rate = dropout_rates[3],\n",
    "            activation=T.tanh\n",
    "        )\n",
    "        else:\n",
    "            layer2 = HiddenLayer(\n",
    "                rng,\n",
    "                input=layer2_input,\n",
    "                n_in=nkerns[1] * 3 * 3,\n",
    "                n_out=150,\n",
    "                W=layer2_W,\n",
    "                b=layer2_b,\n",
    "                activation=T.tanh\n",
    "            )\n",
    "        \n",
    "        layer3 = LogisticRegression(\n",
    "            input=layer2.output, \n",
    "            n_in=150, \n",
    "            n_out=10,\n",
    "            W=layer3_W,\n",
    "            b=layer3_b\n",
    "        )\n",
    "\n",
    "        cost = layer3.negative_log_likelihood(y)\n",
    "        self.test_model = theano.function(\n",
    "            [index],\n",
    "            layer3.errors(y),\n",
    "            givens={\n",
    "                x: test_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "                y: test_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "            }\n",
    "        )\n",
    "        self.test_model = theano.function(\n",
    "            [index],\n",
    "            layer3.errors(y),\n",
    "            givens={\n",
    "                x: test_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "                y: test_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "            }\n",
    "        )\n",
    "        self.valid_output_batch = theano.function(\n",
    "            [index],\n",
    "            layer3.p_y_given_x,\n",
    "            givens={\n",
    "                x: valid_set_x[index * batch_size: (index + 1) * batch_size]\n",
    "            }\n",
    "        )\n",
    "        self.test_output_batch = theano.function(\n",
    "            [index],\n",
    "            layer3.p_y_given_x,\n",
    "            givens={\n",
    "                x: test_set_x[index * batch_size: (index + 1) * batch_size]\n",
    "            }\n",
    "        )\n",
    "        self.validate_model = theano.function(\n",
    "            [index],\n",
    "            layer3.errors(y),\n",
    "            givens={\n",
    "                x: valid_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "                y: valid_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "            }\n",
    "        )\n",
    "        self.params = layer3.params + layer2.params + layer1.params + layer0.params\n",
    "        self.column_params = [nkerns, batch_size, normalized_width, distortion]\n",
    "        self.column_params = [nkerns, batch_size, normalized_width, distortion]\n",
    "\n",
    "        grads  = T.grad(cost, self.params)\n",
    "\n",
    "        updates = [\n",
    "            (param_i, param_i - learning_rate * grad_i)\n",
    "            for param_i, grad_i in zip(self.params, grads)\n",
    "        ]\n",
    "\n",
    "        # train the model\n",
    "        self.train_model = theano.function(\n",
    "            [index, learning_rate],\n",
    "            cost,\n",
    "            updates=updates,\n",
    "            givens={\n",
    "                x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "                y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "invalid-moment",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNColumn(DNNColumn):\n",
    "    def valid_outputs(self):\n",
    "        test_losses = [\n",
    "            self.valid_output_batch(i)\n",
    "            for i in range(self.n_valid_batches)\n",
    "        ]\n",
    "        return numpy.concatenate(test_losses)\n",
    "\n",
    "    def test_outputs(self):\n",
    "        test_losses = [\n",
    "            self.test_output_batch(i)\n",
    "            for i in range(self.n_test_batches)\n",
    "        ]\n",
    "        return numpy.concatenate(test_losses)\n",
    "\n",
    "    def train_column(self, n_epochs=800,init_learning_rate=0.001):\n",
    "        ######################\n",
    "        # TRAIN MODEL COLUMN #\n",
    "        ######################\n",
    "        print('... training')\n",
    "        # early-stopping parameters\n",
    "        patience = 10000\n",
    "        patience_increase = 2  \n",
    "        improvement_threshold = 0.995  \n",
    "        validation_frequency = min(self.n_train_batches, patience / 2)\n",
    "\n",
    "        best_validation_loss = numpy.inf\n",
    "        best_iter = 0\n",
    "        test_score = 0.\n",
    "        start_time = timeit.default_timer()\n",
    "\n",
    "        epoch = 0\n",
    "        done_looping = False\n",
    "        \n",
    "        while (epoch < n_epochs) and (not done_looping):\n",
    "            current_learning_rate = max(numpy.array([init_learning_rate * 0.993**epoch, 0.00003], dtype=numpy.float32))\n",
    "            epoch = epoch + 1\n",
    "\n",
    "            for minibatch_index in range(self.n_train_batches):\n",
    "\n",
    "                iter = (epoch - 1) * self.n_train_batches + minibatch_index\n",
    "\n",
    "                if iter % 100 == 0:\n",
    "                    print('training @ iter = ', iter)\n",
    "                \n",
    "                cost_ij = self.train_model(minibatch_index, current_learning_rate)\n",
    "\n",
    "                if (iter + 1) % validation_frequency == 0:\n",
    "\n",
    "                    validation_losses = [self.validate_model(i) for i\n",
    "                                         in range(self.n_valid_batches)]\n",
    "                    this_validation_loss = numpy.mean(validation_losses)\n",
    "                    print('epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                          (epoch, minibatch_index + 1, self.n_train_batches,\n",
    "                           this_validation_loss * 100.))\n",
    "\n",
    "                    if this_validation_loss < best_validation_loss:\n",
    "\n",
    "                        if this_validation_loss < best_validation_loss *  \\\n",
    "                           improvement_threshold:\n",
    "                            patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                        best_validation_loss = this_validation_loss\n",
    "                        best_iter = iter\n",
    "\n",
    "\n",
    "                if patience <= iter:\n",
    "                    done_looping = True\n",
    "                    break\n",
    "\n",
    "        end_time = timeit.default_timer()\n",
    "        print('Optimization complete.')\n",
    "        print('Best validation score of %f %% obtained at iteration %i, '\n",
    "              'with test performance %f %%' %\n",
    "              (best_validation_loss * 100., best_iter + 1, test_score * 100.))\n",
    "        print(sys.stderr, ('The code for file ' +\n",
    "                              os.path.split(os.path.abspath(''))[1] +\n",
    "                              ' ran for %.2fm' % ((end_time - start_time) / 60.)))\n",
    "\n",
    "    def save(self, filename=None,dropout=False):\n",
    "        \"\"\"\n",
    "        Will need to load last layer W,b to first layer W,b\n",
    "        \"\"\"\n",
    "        name = filename or 'DNN_%iLayers_t%i' % (len(self.params) / 2, int(time.time()))\n",
    "\n",
    "        print('Saving Model as \"%s\"...' % name)\n",
    "        if dropout:\n",
    "            f = open('/Users/Dropoutmodels/'+name+'.pkl', 'wb+')\n",
    "        else:\n",
    "            f = open('/Users/models/'+name+'.pkl', 'wb+')\n",
    "\n",
    "        pickle.dump([param.get_value(borrow=True) for param in self.params], f, -1)\n",
    "        pickle.dump(self.column_params, f, -1)\n",
    "        f.close()\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "hydraulic-queensland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mcdnn_column(normalized_width=0, n_epochs=800, trail=0,dropout=False):\n",
    "    print('... train %i column of normalization %i' % (trail, normalized_width))\n",
    "    print('... num_epochs %i' % (n_epochs))\n",
    "    \n",
    "    datasets = load_data(dataset='mnist.pkl.gz', digit_normalized_width=normalized_width, digit_out_image_size=29)\n",
    "    if dropout:\n",
    "        column = DNNColumn(ds=datasets, normalized_width=normalized_width,dropout=True)\n",
    "    else:\n",
    "        column = DNNColumn(ds=datasets, normalized_width=normalized_width)\n",
    "    column.train_column(n_epochs=n_epochs, init_learning_rate=0.1)\n",
    "    \n",
    "    filename = 'mcdnn_nm%i_trail%i_Layers_time_%i' % (normalized_width, trail, int(time.time()))\n",
    "    column.save(filename,dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-staff",
   "metadata": {},
   "source": [
    "Below we are training the MCDNN in a reduced capacity MCDNN is actually state-of-art on MNIST dataset with an 0,23 error rate. The principle of this algorithm is create a committee of 35 columns pre-trained with this algorithm. On each column we change some aspects of train_set. \n",
    "Our 35 columns divided by:\n",
    "    5 Train per each normalization\n",
    "    7 normalization width: [0,10,12,14,16,18,20]\n",
    "        (0 correspond a dataset without normalization)\n",
    "But below I have only implemented 1 train per normalization due to reduces resources and also it's very time consuming. Originally the number of epoch were also supposed to be 800 but I took it down to 50 which is also the number of epoch I have trained the CNN on.\n",
    "The results doesn't align with the MCDNN claims.\n",
    "The best validitiona score we got in the individual columns is 1.65% but the ensemble of all the columns was greater than 1.8% which is more than the CNN's validation score and the test score was 4.65% which is again more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "exact-loading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... train 0 column of normalization 0\n",
      "... num_epochs 50\n",
      "... loading data\n",
      "... (un)padding digits from 28 -> 29\n",
      "... sharing data\n",
      "... building the dnn column\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-68-5cba53fdea8d>:39: UserWarning: DEPRECATION: the 'ds' parameter is not going to exist anymore as it is going to be replaced by the parameter 'ws'.\n",
      "  pooled_out = pool.pool_2d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... training\n",
      "training @ iter =  0\n",
      "epoch 1, minibatch 100/100, validation error 11.400000 %\n",
      "training @ iter =  100\n",
      "epoch 2, minibatch 100/100, validation error 6.850000 %\n",
      "training @ iter =  200\n",
      "epoch 3, minibatch 100/100, validation error 5.050000 %\n",
      "training @ iter =  300\n",
      "epoch 4, minibatch 100/100, validation error 4.200000 %\n",
      "training @ iter =  400\n",
      "epoch 5, minibatch 100/100, validation error 3.400000 %\n",
      "training @ iter =  500\n",
      "epoch 6, minibatch 100/100, validation error 3.100000 %\n",
      "training @ iter =  600\n",
      "epoch 7, minibatch 100/100, validation error 2.950000 %\n",
      "training @ iter =  700\n",
      "epoch 8, minibatch 100/100, validation error 2.750000 %\n",
      "training @ iter =  800\n",
      "epoch 9, minibatch 100/100, validation error 2.450000 %\n",
      "training @ iter =  900\n",
      "epoch 10, minibatch 100/100, validation error 2.400000 %\n",
      "training @ iter =  1000\n",
      "epoch 11, minibatch 100/100, validation error 2.400000 %\n",
      "training @ iter =  1100\n",
      "epoch 12, minibatch 100/100, validation error 2.400000 %\n",
      "training @ iter =  1200\n",
      "epoch 13, minibatch 100/100, validation error 2.350000 %\n",
      "training @ iter =  1300\n",
      "epoch 14, minibatch 100/100, validation error 2.250000 %\n",
      "training @ iter =  1400\n",
      "epoch 15, minibatch 100/100, validation error 2.200000 %\n",
      "training @ iter =  1500\n",
      "epoch 16, minibatch 100/100, validation error 2.200000 %\n",
      "training @ iter =  1600\n",
      "epoch 17, minibatch 100/100, validation error 2.200000 %\n",
      "training @ iter =  1700\n",
      "epoch 18, minibatch 100/100, validation error 2.150000 %\n",
      "training @ iter =  1800\n",
      "epoch 19, minibatch 100/100, validation error 2.100000 %\n",
      "training @ iter =  1900\n",
      "epoch 20, minibatch 100/100, validation error 2.150000 %\n",
      "training @ iter =  2000\n",
      "epoch 21, minibatch 100/100, validation error 2.150000 %\n",
      "training @ iter =  2100\n",
      "epoch 22, minibatch 100/100, validation error 2.200000 %\n",
      "training @ iter =  2200\n",
      "epoch 23, minibatch 100/100, validation error 2.150000 %\n",
      "training @ iter =  2300\n",
      "epoch 24, minibatch 100/100, validation error 2.150000 %\n",
      "training @ iter =  2400\n",
      "epoch 25, minibatch 100/100, validation error 2.150000 %\n",
      "training @ iter =  2500\n",
      "epoch 26, minibatch 100/100, validation error 2.150000 %\n",
      "training @ iter =  2600\n",
      "epoch 27, minibatch 100/100, validation error 2.100000 %\n",
      "training @ iter =  2700\n",
      "epoch 28, minibatch 100/100, validation error 2.100000 %\n",
      "training @ iter =  2800\n",
      "epoch 29, minibatch 100/100, validation error 2.100000 %\n",
      "training @ iter =  2900\n",
      "epoch 30, minibatch 100/100, validation error 2.100000 %\n",
      "training @ iter =  3000\n",
      "epoch 31, minibatch 100/100, validation error 2.100000 %\n",
      "training @ iter =  3100\n",
      "epoch 32, minibatch 100/100, validation error 2.100000 %\n",
      "training @ iter =  3200\n",
      "epoch 33, minibatch 100/100, validation error 2.100000 %\n",
      "training @ iter =  3300\n",
      "epoch 34, minibatch 100/100, validation error 1.950000 %\n",
      "training @ iter =  3400\n",
      "epoch 35, minibatch 100/100, validation error 1.950000 %\n",
      "training @ iter =  3500\n",
      "epoch 36, minibatch 100/100, validation error 1.950000 %\n",
      "training @ iter =  3600\n",
      "epoch 37, minibatch 100/100, validation error 1.800000 %\n",
      "training @ iter =  3700\n",
      "epoch 38, minibatch 100/100, validation error 1.800000 %\n",
      "training @ iter =  3800\n",
      "epoch 39, minibatch 100/100, validation error 1.850000 %\n",
      "training @ iter =  3900\n",
      "epoch 40, minibatch 100/100, validation error 1.850000 %\n",
      "training @ iter =  4000\n",
      "epoch 41, minibatch 100/100, validation error 1.800000 %\n",
      "training @ iter =  4100\n",
      "epoch 42, minibatch 100/100, validation error 1.800000 %\n",
      "training @ iter =  4200\n",
      "epoch 43, minibatch 100/100, validation error 1.800000 %\n",
      "training @ iter =  4300\n",
      "epoch 44, minibatch 100/100, validation error 1.800000 %\n",
      "training @ iter =  4400\n",
      "epoch 45, minibatch 100/100, validation error 1.750000 %\n",
      "training @ iter =  4500\n",
      "epoch 46, minibatch 100/100, validation error 1.700000 %\n",
      "training @ iter =  4600\n",
      "epoch 47, minibatch 100/100, validation error 1.700000 %\n",
      "training @ iter =  4700\n",
      "epoch 48, minibatch 100/100, validation error 1.700000 %\n",
      "training @ iter =  4800\n",
      "epoch 49, minibatch 100/100, validation error 1.700000 %\n",
      "training @ iter =  4900\n",
      "epoch 50, minibatch 100/100, validation error 1.650000 %\n",
      "Optimization complete.\n",
      "Best validation score of 1.650000 % obtained at iteration 5000, with test performance 0.000000 %\n",
      "<ipykernel.iostream.OutStream object at 0x7ffed0646790> The code for file pragnamandadi ran for 32.92m\n",
      "Saving Model as \"mcdnn_nm0_trail0_Layers_time_1631066744\"...\n",
      "... train 0 column of normalization 10\n",
      "... num_epochs 50\n",
      "... loading data\n",
      "... normalizing digits to width 10 with extra padding 1\n",
      "... sharing data\n",
      "... building the dnn column\n",
      "... training\n",
      "training @ iter =  0\n",
      "epoch 1, minibatch 100/100, validation error 79.750000 %\n",
      "training @ iter =  100\n",
      "epoch 2, minibatch 100/100, validation error 79.700000 %\n",
      "training @ iter =  200\n",
      "epoch 3, minibatch 100/100, validation error 79.350000 %\n",
      "training @ iter =  300\n",
      "epoch 4, minibatch 100/100, validation error 79.200000 %\n",
      "training @ iter =  400\n",
      "epoch 5, minibatch 100/100, validation error 79.200000 %\n",
      "training @ iter =  500\n",
      "epoch 6, minibatch 100/100, validation error 79.100000 %\n",
      "training @ iter =  600\n",
      "epoch 7, minibatch 100/100, validation error 79.000000 %\n",
      "training @ iter =  700\n",
      "epoch 8, minibatch 100/100, validation error 78.850000 %\n",
      "training @ iter =  800\n",
      "epoch 9, minibatch 100/100, validation error 78.800000 %\n",
      "training @ iter =  900\n",
      "epoch 10, minibatch 100/100, validation error 78.650000 %\n",
      "training @ iter =  1000\n",
      "epoch 11, minibatch 100/100, validation error 78.650000 %\n",
      "training @ iter =  1100\n",
      "epoch 12, minibatch 100/100, validation error 78.550000 %\n",
      "training @ iter =  1200\n",
      "epoch 13, minibatch 100/100, validation error 78.550000 %\n",
      "training @ iter =  1300\n",
      "epoch 14, minibatch 100/100, validation error 78.550000 %\n",
      "training @ iter =  1400\n",
      "epoch 15, minibatch 100/100, validation error 78.550000 %\n",
      "training @ iter =  1500\n",
      "epoch 16, minibatch 100/100, validation error 78.550000 %\n",
      "training @ iter =  1600\n",
      "epoch 17, minibatch 100/100, validation error 78.550000 %\n",
      "training @ iter =  1700\n",
      "epoch 18, minibatch 100/100, validation error 78.550000 %\n",
      "training @ iter =  1800\n",
      "epoch 19, minibatch 100/100, validation error 78.550000 %\n",
      "training @ iter =  1900\n",
      "epoch 20, minibatch 100/100, validation error 78.500000 %\n",
      "training @ iter =  2000\n",
      "epoch 21, minibatch 100/100, validation error 78.500000 %\n",
      "training @ iter =  2100\n",
      "epoch 22, minibatch 100/100, validation error 78.450000 %\n",
      "training @ iter =  2200\n",
      "epoch 23, minibatch 100/100, validation error 78.450000 %\n",
      "training @ iter =  2300\n",
      "epoch 24, minibatch 100/100, validation error 78.450000 %\n",
      "training @ iter =  2400\n",
      "epoch 25, minibatch 100/100, validation error 78.450000 %\n",
      "training @ iter =  2500\n",
      "epoch 26, minibatch 100/100, validation error 78.400000 %\n",
      "training @ iter =  2600\n",
      "epoch 27, minibatch 100/100, validation error 78.050000 %\n",
      "training @ iter =  2700\n",
      "epoch 28, minibatch 100/100, validation error 77.600000 %\n",
      "training @ iter =  2800\n",
      "epoch 29, minibatch 100/100, validation error 76.750000 %\n",
      "training @ iter =  2900\n",
      "epoch 30, minibatch 100/100, validation error 74.600000 %\n",
      "training @ iter =  3000\n",
      "epoch 31, minibatch 100/100, validation error 71.800000 %\n",
      "training @ iter =  3100\n",
      "epoch 32, minibatch 100/100, validation error 67.350000 %\n",
      "training @ iter =  3200\n",
      "epoch 33, minibatch 100/100, validation error 62.000000 %\n",
      "training @ iter =  3300\n",
      "epoch 34, minibatch 100/100, validation error 56.450000 %\n",
      "training @ iter =  3400\n",
      "epoch 35, minibatch 100/100, validation error 51.850000 %\n",
      "training @ iter =  3500\n",
      "epoch 36, minibatch 100/100, validation error 49.650000 %\n",
      "training @ iter =  3600\n",
      "epoch 37, minibatch 100/100, validation error 48.900000 %\n",
      "training @ iter =  3700\n",
      "epoch 38, minibatch 100/100, validation error 48.050000 %\n",
      "training @ iter =  3800\n",
      "epoch 39, minibatch 100/100, validation error 46.250000 %\n",
      "training @ iter =  3900\n",
      "epoch 40, minibatch 100/100, validation error 43.250000 %\n",
      "training @ iter =  4000\n",
      "epoch 41, minibatch 100/100, validation error 38.950000 %\n",
      "training @ iter =  4100\n",
      "epoch 42, minibatch 100/100, validation error 34.900000 %\n",
      "training @ iter =  4200\n",
      "epoch 43, minibatch 100/100, validation error 32.900000 %\n",
      "training @ iter =  4300\n",
      "epoch 44, minibatch 100/100, validation error 31.850000 %\n",
      "training @ iter =  4400\n",
      "epoch 45, minibatch 100/100, validation error 30.550000 %\n",
      "training @ iter =  4500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 46, minibatch 100/100, validation error 28.650000 %\n",
      "training @ iter =  4600\n",
      "epoch 47, minibatch 100/100, validation error 26.000000 %\n",
      "training @ iter =  4700\n",
      "epoch 48, minibatch 100/100, validation error 23.900000 %\n",
      "training @ iter =  4800\n",
      "epoch 49, minibatch 100/100, validation error 22.300000 %\n",
      "training @ iter =  4900\n",
      "epoch 50, minibatch 100/100, validation error 20.400000 %\n",
      "Optimization complete.\n",
      "Best validation score of 20.400000 % obtained at iteration 5000, with test performance 0.000000 %\n",
      "<ipykernel.iostream.OutStream object at 0x7ffed0646790> The code for file pragnamandadi ran for 51.01m\n",
      "Saving Model as \"mcdnn_nm10_trail0_Layers_time_1631069810\"...\n",
      "... train 0 column of normalization 12\n",
      "... num_epochs 50\n",
      "... loading data\n",
      "... normalizing digits to width 12 with extra padding 1\n",
      "... sharing data\n",
      "... building the dnn column\n",
      "... training\n",
      "training @ iter =  0\n",
      "epoch 1, minibatch 100/100, validation error 79.050000 %\n",
      "training @ iter =  100\n",
      "epoch 2, minibatch 100/100, validation error 77.500000 %\n",
      "training @ iter =  200\n",
      "epoch 3, minibatch 100/100, validation error 77.200000 %\n",
      "training @ iter =  300\n",
      "epoch 4, minibatch 100/100, validation error 75.950000 %\n",
      "training @ iter =  400\n",
      "epoch 5, minibatch 100/100, validation error 75.350000 %\n",
      "training @ iter =  500\n",
      "epoch 6, minibatch 100/100, validation error 67.150000 %\n",
      "training @ iter =  600\n",
      "epoch 7, minibatch 100/100, validation error 66.000000 %\n",
      "training @ iter =  700\n",
      "epoch 8, minibatch 100/100, validation error 66.000000 %\n",
      "training @ iter =  800\n",
      "epoch 9, minibatch 100/100, validation error 67.650000 %\n",
      "training @ iter =  900\n",
      "epoch 10, minibatch 100/100, validation error 70.400000 %\n",
      "training @ iter =  1000\n",
      "epoch 11, minibatch 100/100, validation error 72.100000 %\n",
      "training @ iter =  1100\n",
      "epoch 12, minibatch 100/100, validation error 72.600000 %\n",
      "training @ iter =  1200\n",
      "epoch 13, minibatch 100/100, validation error 72.100000 %\n",
      "training @ iter =  1300\n",
      "epoch 14, minibatch 100/100, validation error 70.600000 %\n",
      "training @ iter =  1400\n",
      "epoch 15, minibatch 100/100, validation error 66.200000 %\n",
      "training @ iter =  1500\n",
      "epoch 16, minibatch 100/100, validation error 61.800000 %\n",
      "training @ iter =  1600\n",
      "epoch 17, minibatch 100/100, validation error 57.900000 %\n",
      "training @ iter =  1700\n",
      "epoch 18, minibatch 100/100, validation error 54.250000 %\n",
      "training @ iter =  1800\n",
      "epoch 19, minibatch 100/100, validation error 51.350000 %\n",
      "training @ iter =  1900\n",
      "epoch 20, minibatch 100/100, validation error 47.750000 %\n",
      "training @ iter =  2000\n",
      "epoch 21, minibatch 100/100, validation error 43.900000 %\n",
      "training @ iter =  2100\n",
      "epoch 22, minibatch 100/100, validation error 40.350000 %\n",
      "training @ iter =  2200\n",
      "epoch 23, minibatch 100/100, validation error 36.500000 %\n",
      "training @ iter =  2300\n",
      "epoch 24, minibatch 100/100, validation error 32.200000 %\n",
      "training @ iter =  2400\n",
      "epoch 25, minibatch 100/100, validation error 28.250000 %\n",
      "training @ iter =  2500\n",
      "epoch 26, minibatch 100/100, validation error 23.900000 %\n",
      "training @ iter =  2600\n",
      "epoch 27, minibatch 100/100, validation error 20.800000 %\n",
      "training @ iter =  2700\n",
      "epoch 28, minibatch 100/100, validation error 18.350000 %\n",
      "training @ iter =  2800\n",
      "epoch 29, minibatch 100/100, validation error 17.200000 %\n",
      "training @ iter =  2900\n",
      "epoch 30, minibatch 100/100, validation error 15.900000 %\n",
      "training @ iter =  3000\n",
      "epoch 31, minibatch 100/100, validation error 14.800000 %\n",
      "training @ iter =  3100\n",
      "epoch 32, minibatch 100/100, validation error 13.600000 %\n",
      "training @ iter =  3200\n",
      "epoch 33, minibatch 100/100, validation error 12.400000 %\n",
      "training @ iter =  3300\n",
      "epoch 34, minibatch 100/100, validation error 11.750000 %\n",
      "training @ iter =  3400\n",
      "epoch 35, minibatch 100/100, validation error 11.250000 %\n",
      "training @ iter =  3500\n",
      "epoch 36, minibatch 100/100, validation error 10.500000 %\n",
      "training @ iter =  3600\n",
      "epoch 37, minibatch 100/100, validation error 10.150000 %\n",
      "training @ iter =  3700\n",
      "epoch 38, minibatch 100/100, validation error 9.600000 %\n",
      "training @ iter =  3800\n",
      "epoch 39, minibatch 100/100, validation error 9.450000 %\n",
      "training @ iter =  3900\n",
      "epoch 40, minibatch 100/100, validation error 8.850000 %\n",
      "training @ iter =  4000\n",
      "epoch 41, minibatch 100/100, validation error 8.500000 %\n",
      "training @ iter =  4100\n",
      "epoch 42, minibatch 100/100, validation error 8.150000 %\n",
      "training @ iter =  4200\n",
      "epoch 43, minibatch 100/100, validation error 7.850000 %\n",
      "training @ iter =  4300\n",
      "epoch 44, minibatch 100/100, validation error 7.500000 %\n",
      "training @ iter =  4400\n",
      "epoch 45, minibatch 100/100, validation error 7.300000 %\n",
      "training @ iter =  4500\n",
      "epoch 46, minibatch 100/100, validation error 6.900000 %\n",
      "training @ iter =  4600\n",
      "epoch 47, minibatch 100/100, validation error 6.950000 %\n",
      "training @ iter =  4700\n",
      "epoch 48, minibatch 100/100, validation error 6.800000 %\n",
      "training @ iter =  4800\n",
      "epoch 49, minibatch 100/100, validation error 6.600000 %\n",
      "training @ iter =  4900\n",
      "epoch 50, minibatch 100/100, validation error 6.650000 %\n",
      "Optimization complete.\n",
      "Best validation score of 6.600000 % obtained at iteration 4900, with test performance 0.000000 %\n",
      "<ipykernel.iostream.OutStream object at 0x7ffed0646790> The code for file pragnamandadi ran for 40.27m\n",
      "Saving Model as \"mcdnn_nm12_trail0_Layers_time_1631072234\"...\n",
      "... train 0 column of normalization 14\n",
      "... num_epochs 50\n",
      "... loading data\n",
      "... normalizing digits to width 14 with extra padding 1\n",
      "... sharing data\n",
      "... building the dnn column\n",
      "... training\n",
      "training @ iter =  0\n",
      "epoch 1, minibatch 100/100, validation error 75.850000 %\n",
      "training @ iter =  100\n",
      "epoch 2, minibatch 100/100, validation error 73.350000 %\n",
      "training @ iter =  200\n",
      "epoch 3, minibatch 100/100, validation error 70.150000 %\n",
      "training @ iter =  300\n",
      "epoch 4, minibatch 100/100, validation error 69.400000 %\n",
      "training @ iter =  400\n",
      "epoch 5, minibatch 100/100, validation error 68.250000 %\n",
      "training @ iter =  500\n",
      "epoch 6, minibatch 100/100, validation error 61.550000 %\n",
      "training @ iter =  600\n",
      "epoch 7, minibatch 100/100, validation error 60.350000 %\n",
      "training @ iter =  700\n",
      "epoch 8, minibatch 100/100, validation error 58.900000 %\n",
      "training @ iter =  800\n",
      "epoch 9, minibatch 100/100, validation error 49.700000 %\n",
      "training @ iter =  900\n",
      "epoch 10, minibatch 100/100, validation error 48.800000 %\n",
      "training @ iter =  1000\n",
      "epoch 11, minibatch 100/100, validation error 51.150000 %\n",
      "training @ iter =  1100\n",
      "epoch 12, minibatch 100/100, validation error 52.000000 %\n",
      "training @ iter =  1200\n",
      "epoch 13, minibatch 100/100, validation error 49.800000 %\n",
      "training @ iter =  1300\n",
      "epoch 14, minibatch 100/100, validation error 45.250000 %\n",
      "training @ iter =  1400\n",
      "epoch 15, minibatch 100/100, validation error 39.750000 %\n",
      "training @ iter =  1500\n",
      "epoch 16, minibatch 100/100, validation error 32.900000 %\n",
      "training @ iter =  1600\n",
      "epoch 17, minibatch 100/100, validation error 26.750000 %\n",
      "training @ iter =  1700\n",
      "epoch 18, minibatch 100/100, validation error 23.150000 %\n",
      "training @ iter =  1800\n",
      "epoch 19, minibatch 100/100, validation error 20.350000 %\n",
      "training @ iter =  1900\n",
      "epoch 20, minibatch 100/100, validation error 17.700000 %\n",
      "training @ iter =  2000\n",
      "epoch 21, minibatch 100/100, validation error 15.850000 %\n",
      "training @ iter =  2100\n",
      "epoch 22, minibatch 100/100, validation error 14.750000 %\n",
      "training @ iter =  2200\n",
      "epoch 23, minibatch 100/100, validation error 14.250000 %\n",
      "training @ iter =  2300\n",
      "epoch 24, minibatch 100/100, validation error 14.100000 %\n",
      "training @ iter =  2400\n",
      "epoch 25, minibatch 100/100, validation error 13.550000 %\n",
      "training @ iter =  2500\n",
      "epoch 26, minibatch 100/100, validation error 12.550000 %\n",
      "training @ iter =  2600\n",
      "epoch 27, minibatch 100/100, validation error 11.550000 %\n",
      "training @ iter =  2700\n",
      "epoch 28, minibatch 100/100, validation error 11.100000 %\n",
      "training @ iter =  2800\n",
      "epoch 29, minibatch 100/100, validation error 10.150000 %\n",
      "training @ iter =  2900\n",
      "epoch 30, minibatch 100/100, validation error 9.850000 %\n",
      "training @ iter =  3000\n",
      "epoch 31, minibatch 100/100, validation error 9.350000 %\n",
      "training @ iter =  3100\n",
      "epoch 32, minibatch 100/100, validation error 9.050000 %\n",
      "training @ iter =  3200\n",
      "epoch 33, minibatch 100/100, validation error 8.650000 %\n",
      "training @ iter =  3300\n",
      "epoch 34, minibatch 100/100, validation error 8.400000 %\n",
      "training @ iter =  3400\n",
      "epoch 35, minibatch 100/100, validation error 8.150000 %\n",
      "training @ iter =  3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 36, minibatch 100/100, validation error 7.800000 %\n",
      "training @ iter =  3600\n",
      "epoch 37, minibatch 100/100, validation error 7.400000 %\n",
      "training @ iter =  3700\n",
      "epoch 38, minibatch 100/100, validation error 7.200000 %\n",
      "training @ iter =  3800\n",
      "epoch 39, minibatch 100/100, validation error 7.150000 %\n",
      "training @ iter =  3900\n",
      "epoch 40, minibatch 100/100, validation error 7.000000 %\n",
      "training @ iter =  4000\n",
      "epoch 41, minibatch 100/100, validation error 6.900000 %\n",
      "training @ iter =  4100\n",
      "epoch 42, minibatch 100/100, validation error 6.750000 %\n",
      "training @ iter =  4200\n",
      "epoch 43, minibatch 100/100, validation error 6.550000 %\n",
      "training @ iter =  4300\n",
      "epoch 44, minibatch 100/100, validation error 6.550000 %\n",
      "training @ iter =  4400\n",
      "epoch 45, minibatch 100/100, validation error 6.350000 %\n",
      "training @ iter =  4500\n",
      "epoch 46, minibatch 100/100, validation error 6.200000 %\n",
      "training @ iter =  4600\n",
      "epoch 47, minibatch 100/100, validation error 6.050000 %\n",
      "training @ iter =  4700\n",
      "epoch 48, minibatch 100/100, validation error 5.850000 %\n",
      "training @ iter =  4800\n",
      "epoch 49, minibatch 100/100, validation error 5.650000 %\n",
      "training @ iter =  4900\n",
      "epoch 50, minibatch 100/100, validation error 5.500000 %\n",
      "Optimization complete.\n",
      "Best validation score of 5.500000 % obtained at iteration 5000, with test performance 0.000000 %\n",
      "<ipykernel.iostream.OutStream object at 0x7ffed0646790> The code for file pragnamandadi ran for 39.52m\n",
      "Saving Model as \"mcdnn_nm14_trail0_Layers_time_1631074611\"...\n",
      "... train 0 column of normalization 16\n",
      "... num_epochs 50\n",
      "... loading data\n",
      "... normalizing digits to width 16 with extra padding 1\n",
      "... sharing data\n",
      "... building the dnn column\n",
      "... training\n",
      "training @ iter =  0\n",
      "epoch 1, minibatch 100/100, validation error 74.600000 %\n",
      "training @ iter =  100\n",
      "epoch 2, minibatch 100/100, validation error 71.600000 %\n",
      "training @ iter =  200\n",
      "epoch 3, minibatch 100/100, validation error 68.850000 %\n",
      "training @ iter =  300\n",
      "epoch 4, minibatch 100/100, validation error 68.000000 %\n",
      "training @ iter =  400\n",
      "epoch 5, minibatch 100/100, validation error 67.450000 %\n",
      "training @ iter =  500\n",
      "epoch 6, minibatch 100/100, validation error 67.000000 %\n",
      "training @ iter =  600\n",
      "epoch 7, minibatch 100/100, validation error 66.700000 %\n",
      "training @ iter =  700\n",
      "epoch 8, minibatch 100/100, validation error 66.100000 %\n",
      "training @ iter =  800\n",
      "epoch 9, minibatch 100/100, validation error 62.000000 %\n",
      "training @ iter =  900\n",
      "epoch 10, minibatch 100/100, validation error 58.500000 %\n",
      "training @ iter =  1000\n",
      "epoch 11, minibatch 100/100, validation error 51.600000 %\n",
      "training @ iter =  1100\n",
      "epoch 12, minibatch 100/100, validation error 42.800000 %\n",
      "training @ iter =  1200\n",
      "epoch 13, minibatch 100/100, validation error 34.250000 %\n",
      "training @ iter =  1300\n",
      "epoch 14, minibatch 100/100, validation error 27.650000 %\n",
      "training @ iter =  1400\n",
      "epoch 15, minibatch 100/100, validation error 22.250000 %\n",
      "training @ iter =  1500\n",
      "epoch 16, minibatch 100/100, validation error 18.650000 %\n",
      "training @ iter =  1600\n",
      "epoch 17, minibatch 100/100, validation error 15.800000 %\n",
      "training @ iter =  1700\n",
      "epoch 18, minibatch 100/100, validation error 14.300000 %\n",
      "training @ iter =  1800\n",
      "epoch 19, minibatch 100/100, validation error 14.050000 %\n",
      "training @ iter =  1900\n",
      "epoch 20, minibatch 100/100, validation error 13.100000 %\n",
      "training @ iter =  2000\n",
      "epoch 21, minibatch 100/100, validation error 12.200000 %\n",
      "training @ iter =  2100\n",
      "epoch 22, minibatch 100/100, validation error 11.100000 %\n",
      "training @ iter =  2200\n",
      "epoch 23, minibatch 100/100, validation error 10.150000 %\n",
      "training @ iter =  2300\n",
      "epoch 24, minibatch 100/100, validation error 9.650000 %\n",
      "training @ iter =  2400\n",
      "epoch 25, minibatch 100/100, validation error 9.200000 %\n",
      "training @ iter =  2500\n",
      "epoch 26, minibatch 100/100, validation error 8.850000 %\n",
      "training @ iter =  2600\n",
      "epoch 27, minibatch 100/100, validation error 8.450000 %\n",
      "training @ iter =  2700\n",
      "epoch 28, minibatch 100/100, validation error 8.150000 %\n",
      "training @ iter =  2800\n",
      "epoch 29, minibatch 100/100, validation error 7.900000 %\n",
      "training @ iter =  2900\n",
      "epoch 30, minibatch 100/100, validation error 7.800000 %\n",
      "training @ iter =  3000\n",
      "epoch 31, minibatch 100/100, validation error 7.400000 %\n",
      "training @ iter =  3100\n",
      "epoch 32, minibatch 100/100, validation error 7.300000 %\n",
      "training @ iter =  3200\n",
      "epoch 33, minibatch 100/100, validation error 7.250000 %\n",
      "training @ iter =  3300\n",
      "epoch 34, minibatch 100/100, validation error 7.100000 %\n",
      "training @ iter =  3400\n",
      "epoch 35, minibatch 100/100, validation error 6.800000 %\n",
      "training @ iter =  3500\n",
      "epoch 36, minibatch 100/100, validation error 6.850000 %\n",
      "training @ iter =  3600\n",
      "epoch 37, minibatch 100/100, validation error 6.800000 %\n",
      "training @ iter =  3700\n",
      "epoch 38, minibatch 100/100, validation error 6.750000 %\n",
      "training @ iter =  3800\n",
      "epoch 39, minibatch 100/100, validation error 6.700000 %\n",
      "training @ iter =  3900\n",
      "epoch 40, minibatch 100/100, validation error 6.650000 %\n",
      "training @ iter =  4000\n",
      "epoch 41, minibatch 100/100, validation error 6.500000 %\n",
      "training @ iter =  4100\n",
      "epoch 42, minibatch 100/100, validation error 6.350000 %\n",
      "training @ iter =  4200\n",
      "epoch 43, minibatch 100/100, validation error 6.200000 %\n",
      "training @ iter =  4300\n",
      "epoch 44, minibatch 100/100, validation error 6.200000 %\n",
      "training @ iter =  4400\n",
      "epoch 45, minibatch 100/100, validation error 6.000000 %\n",
      "training @ iter =  4500\n",
      "epoch 46, minibatch 100/100, validation error 5.900000 %\n",
      "training @ iter =  4600\n",
      "epoch 47, minibatch 100/100, validation error 5.850000 %\n",
      "training @ iter =  4700\n",
      "epoch 48, minibatch 100/100, validation error 5.650000 %\n",
      "training @ iter =  4800\n",
      "epoch 49, minibatch 100/100, validation error 5.400000 %\n",
      "training @ iter =  4900\n",
      "epoch 50, minibatch 100/100, validation error 5.250000 %\n",
      "Optimization complete.\n",
      "Best validation score of 5.250000 % obtained at iteration 5000, with test performance 0.000000 %\n",
      "<ipykernel.iostream.OutStream object at 0x7ffed0646790> The code for file pragnamandadi ran for 44.43m\n",
      "Saving Model as \"mcdnn_nm16_trail0_Layers_time_1631077284\"...\n",
      "... train 0 column of normalization 18\n",
      "... num_epochs 50\n",
      "... loading data\n",
      "... normalizing digits to width 18 with extra padding 1\n",
      "... sharing data\n",
      "... building the dnn column\n",
      "... training\n",
      "training @ iter =  0\n",
      "epoch 1, minibatch 100/100, validation error 76.150000 %\n",
      "training @ iter =  100\n",
      "epoch 2, minibatch 100/100, validation error 74.600000 %\n",
      "training @ iter =  200\n",
      "epoch 3, minibatch 100/100, validation error 73.500000 %\n",
      "training @ iter =  300\n",
      "epoch 4, minibatch 100/100, validation error 72.500000 %\n",
      "training @ iter =  400\n",
      "epoch 5, minibatch 100/100, validation error 71.800000 %\n",
      "training @ iter =  500\n",
      "epoch 6, minibatch 100/100, validation error 69.500000 %\n",
      "training @ iter =  600\n",
      "epoch 7, minibatch 100/100, validation error 63.000000 %\n",
      "training @ iter =  700\n",
      "epoch 8, minibatch 100/100, validation error 60.650000 %\n",
      "training @ iter =  800\n",
      "epoch 9, minibatch 100/100, validation error 54.950000 %\n",
      "training @ iter =  900\n",
      "epoch 10, minibatch 100/100, validation error 49.200000 %\n",
      "training @ iter =  1000\n",
      "epoch 11, minibatch 100/100, validation error 41.400000 %\n",
      "training @ iter =  1100\n",
      "epoch 12, minibatch 100/100, validation error 34.250000 %\n",
      "training @ iter =  1200\n",
      "epoch 13, minibatch 100/100, validation error 28.850000 %\n",
      "training @ iter =  1300\n",
      "epoch 14, minibatch 100/100, validation error 27.450000 %\n",
      "training @ iter =  1400\n",
      "epoch 15, minibatch 100/100, validation error 23.550000 %\n",
      "training @ iter =  1500\n",
      "epoch 16, minibatch 100/100, validation error 19.900000 %\n",
      "training @ iter =  1600\n",
      "epoch 17, minibatch 100/100, validation error 17.550000 %\n",
      "training @ iter =  1700\n",
      "epoch 18, minibatch 100/100, validation error 15.900000 %\n",
      "training @ iter =  1800\n",
      "epoch 19, minibatch 100/100, validation error 14.750000 %\n",
      "training @ iter =  1900\n",
      "epoch 20, minibatch 100/100, validation error 14.000000 %\n",
      "training @ iter =  2000\n",
      "epoch 21, minibatch 100/100, validation error 12.600000 %\n",
      "training @ iter =  2100\n",
      "epoch 22, minibatch 100/100, validation error 11.650000 %\n",
      "training @ iter =  2200\n",
      "epoch 23, minibatch 100/100, validation error 10.900000 %\n",
      "training @ iter =  2300\n",
      "epoch 24, minibatch 100/100, validation error 10.300000 %\n",
      "training @ iter =  2400\n",
      "epoch 25, minibatch 100/100, validation error 9.850000 %\n",
      "training @ iter =  2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 26, minibatch 100/100, validation error 9.700000 %\n",
      "training @ iter =  2600\n",
      "epoch 27, minibatch 100/100, validation error 9.450000 %\n",
      "training @ iter =  2700\n",
      "epoch 28, minibatch 100/100, validation error 9.150000 %\n",
      "training @ iter =  2800\n",
      "epoch 29, minibatch 100/100, validation error 8.750000 %\n",
      "training @ iter =  2900\n",
      "epoch 30, minibatch 100/100, validation error 8.500000 %\n",
      "training @ iter =  3000\n",
      "epoch 31, minibatch 100/100, validation error 8.200000 %\n",
      "training @ iter =  3100\n",
      "epoch 32, minibatch 100/100, validation error 7.900000 %\n",
      "training @ iter =  3200\n",
      "epoch 33, minibatch 100/100, validation error 7.700000 %\n",
      "training @ iter =  3300\n",
      "epoch 34, minibatch 100/100, validation error 7.350000 %\n",
      "training @ iter =  3400\n",
      "epoch 35, minibatch 100/100, validation error 7.150000 %\n",
      "training @ iter =  3500\n",
      "epoch 36, minibatch 100/100, validation error 7.050000 %\n",
      "training @ iter =  3600\n",
      "epoch 37, minibatch 100/100, validation error 6.850000 %\n",
      "training @ iter =  3700\n",
      "epoch 38, minibatch 100/100, validation error 6.800000 %\n",
      "training @ iter =  3800\n",
      "epoch 39, minibatch 100/100, validation error 6.500000 %\n",
      "training @ iter =  3900\n",
      "epoch 40, minibatch 100/100, validation error 6.350000 %\n",
      "training @ iter =  4000\n",
      "epoch 41, minibatch 100/100, validation error 6.150000 %\n",
      "training @ iter =  4100\n",
      "epoch 42, minibatch 100/100, validation error 5.950000 %\n",
      "training @ iter =  4200\n",
      "epoch 43, minibatch 100/100, validation error 5.800000 %\n",
      "training @ iter =  4300\n",
      "epoch 44, minibatch 100/100, validation error 5.750000 %\n",
      "training @ iter =  4400\n",
      "epoch 45, minibatch 100/100, validation error 5.600000 %\n",
      "training @ iter =  4500\n",
      "epoch 46, minibatch 100/100, validation error 5.550000 %\n",
      "training @ iter =  4600\n",
      "epoch 47, minibatch 100/100, validation error 5.450000 %\n",
      "training @ iter =  4700\n",
      "epoch 48, minibatch 100/100, validation error 5.400000 %\n",
      "training @ iter =  4800\n",
      "epoch 49, minibatch 100/100, validation error 5.300000 %\n",
      "training @ iter =  4900\n",
      "epoch 50, minibatch 100/100, validation error 5.250000 %\n",
      "Optimization complete.\n",
      "Best validation score of 5.250000 % obtained at iteration 5000, with test performance 0.000000 %\n",
      "<ipykernel.iostream.OutStream object at 0x7ffed0646790> The code for file pragnamandadi ran for 31.98m\n",
      "Saving Model as \"mcdnn_nm18_trail0_Layers_time_1631079209\"...\n",
      "... train 0 column of normalization 20\n",
      "... num_epochs 50\n",
      "... loading data\n",
      "... normalizing digits to width 20 with extra padding 1\n",
      "... sharing data\n",
      "... building the dnn column\n",
      "... training\n",
      "training @ iter =  0\n",
      "epoch 1, minibatch 100/100, validation error 68.850000 %\n",
      "training @ iter =  100\n",
      "epoch 2, minibatch 100/100, validation error 65.300000 %\n",
      "training @ iter =  200\n",
      "epoch 3, minibatch 100/100, validation error 63.650000 %\n",
      "training @ iter =  300\n",
      "epoch 4, minibatch 100/100, validation error 63.350000 %\n",
      "training @ iter =  400\n",
      "epoch 5, minibatch 100/100, validation error 63.000000 %\n",
      "training @ iter =  500\n",
      "epoch 6, minibatch 100/100, validation error 60.400000 %\n",
      "training @ iter =  600\n",
      "epoch 7, minibatch 100/100, validation error 57.650000 %\n",
      "training @ iter =  700\n",
      "epoch 8, minibatch 100/100, validation error 55.850000 %\n",
      "training @ iter =  800\n",
      "epoch 9, minibatch 100/100, validation error 55.000000 %\n",
      "training @ iter =  900\n",
      "epoch 10, minibatch 100/100, validation error 54.500000 %\n",
      "training @ iter =  1000\n",
      "epoch 11, minibatch 100/100, validation error 53.350000 %\n",
      "training @ iter =  1100\n",
      "epoch 12, minibatch 100/100, validation error 48.450000 %\n",
      "training @ iter =  1200\n",
      "epoch 13, minibatch 100/100, validation error 40.300000 %\n",
      "training @ iter =  1300\n",
      "epoch 14, minibatch 100/100, validation error 27.500000 %\n",
      "training @ iter =  1400\n",
      "epoch 15, minibatch 100/100, validation error 19.300000 %\n",
      "training @ iter =  1500\n",
      "epoch 16, minibatch 100/100, validation error 15.200000 %\n",
      "training @ iter =  1600\n",
      "epoch 17, minibatch 100/100, validation error 12.300000 %\n",
      "training @ iter =  1700\n",
      "epoch 18, minibatch 100/100, validation error 11.250000 %\n",
      "training @ iter =  1800\n",
      "epoch 19, minibatch 100/100, validation error 10.350000 %\n",
      "training @ iter =  1900\n",
      "epoch 20, minibatch 100/100, validation error 9.650000 %\n",
      "training @ iter =  2000\n",
      "epoch 21, minibatch 100/100, validation error 9.200000 %\n",
      "training @ iter =  2100\n",
      "epoch 22, minibatch 100/100, validation error 8.550000 %\n",
      "training @ iter =  2200\n",
      "epoch 23, minibatch 100/100, validation error 8.450000 %\n",
      "training @ iter =  2300\n",
      "epoch 24, minibatch 100/100, validation error 7.800000 %\n",
      "training @ iter =  2400\n",
      "epoch 25, minibatch 100/100, validation error 7.550000 %\n",
      "training @ iter =  2500\n",
      "epoch 26, minibatch 100/100, validation error 7.300000 %\n",
      "training @ iter =  2600\n",
      "epoch 27, minibatch 100/100, validation error 7.150000 %\n",
      "training @ iter =  2700\n",
      "epoch 28, minibatch 100/100, validation error 6.800000 %\n",
      "training @ iter =  2800\n",
      "epoch 29, minibatch 100/100, validation error 6.700000 %\n",
      "training @ iter =  2900\n",
      "epoch 30, minibatch 100/100, validation error 6.550000 %\n",
      "training @ iter =  3000\n",
      "epoch 31, minibatch 100/100, validation error 6.450000 %\n",
      "training @ iter =  3100\n",
      "epoch 32, minibatch 100/100, validation error 6.300000 %\n",
      "training @ iter =  3200\n",
      "epoch 33, minibatch 100/100, validation error 6.250000 %\n",
      "training @ iter =  3300\n",
      "epoch 34, minibatch 100/100, validation error 6.200000 %\n",
      "training @ iter =  3400\n",
      "epoch 35, minibatch 100/100, validation error 6.000000 %\n",
      "training @ iter =  3500\n",
      "epoch 36, minibatch 100/100, validation error 5.900000 %\n",
      "training @ iter =  3600\n",
      "epoch 37, minibatch 100/100, validation error 5.600000 %\n",
      "training @ iter =  3700\n",
      "epoch 38, minibatch 100/100, validation error 5.500000 %\n",
      "training @ iter =  3800\n",
      "epoch 39, minibatch 100/100, validation error 5.350000 %\n",
      "training @ iter =  3900\n",
      "epoch 40, minibatch 100/100, validation error 5.150000 %\n",
      "training @ iter =  4000\n",
      "epoch 41, minibatch 100/100, validation error 4.900000 %\n",
      "training @ iter =  4100\n",
      "epoch 42, minibatch 100/100, validation error 4.800000 %\n",
      "training @ iter =  4200\n",
      "epoch 43, minibatch 100/100, validation error 4.700000 %\n",
      "training @ iter =  4300\n",
      "epoch 44, minibatch 100/100, validation error 4.700000 %\n",
      "training @ iter =  4400\n",
      "epoch 45, minibatch 100/100, validation error 4.600000 %\n",
      "training @ iter =  4500\n",
      "epoch 46, minibatch 100/100, validation error 4.500000 %\n",
      "training @ iter =  4600\n",
      "epoch 47, minibatch 100/100, validation error 4.350000 %\n",
      "training @ iter =  4700\n",
      "epoch 48, minibatch 100/100, validation error 4.300000 %\n",
      "training @ iter =  4800\n",
      "epoch 49, minibatch 100/100, validation error 4.250000 %\n",
      "training @ iter =  4900\n",
      "epoch 50, minibatch 100/100, validation error 4.250000 %\n",
      "Optimization complete.\n",
      "Best validation score of 4.250000 % obtained at iteration 4900, with test performance 0.000000 %\n",
      "<ipykernel.iostream.OutStream object at 0x7ffed0646790> The code for file pragnamandadi ran for 31.76m\n",
      "Saving Model as \"mcdnn_nm20_trail0_Layers_time_1631081119\"...\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "for nm in [0,10,12,14,16,18,20]:\n",
    "    train_mcdnn_column(nm, n_epochs=50, trail=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "variable-scenario",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import glob\n",
    "def test_columns():\n",
    "    dataset='mnist.pkl.gz'\n",
    "    # create data hash that will be filled with data from different normalizations\n",
    "    all_datasets = {}\n",
    "    # instantiate multiple columns\n",
    "    columns = []\n",
    "    models = files = glob.glob('/Users/models/' + \"/*.pkl\")\n",
    "    print('... Starting to test %i columns' % len(models))\n",
    "    print(models)\n",
    "    for model in models:\n",
    "        # load model params\n",
    "        f = open(model,'rb')\n",
    "        u = pickle._Unpickler(f)\n",
    "        u.encoding = 'latin1'\n",
    "        params = u.load()\n",
    "        nkerns, batch_size, normalized_width, distortion = pickle.load(f)\n",
    "        if all_datasets.get(normalized_width):\n",
    "            datasets = all_datasets[normalized_width]\n",
    "        else:\n",
    "            datasets = load_data(dataset, normalized_width, 29)\n",
    "            all_datasets[normalized_width] = datasets\n",
    "        # no distortion during testing\n",
    "        columns.append(DNNColumn(datasets, nkerns, batch_size, normalized_width, 0, params))\n",
    "    print('... Forward propagating %i columns' % len(models))\n",
    "    # call test on all of them recieving 10 outputs\n",
    "#     if valid_test=='V':\n",
    "#         model_outputs = [column.valid_outputs() for column in columns] \n",
    "#         position_ds   = 1 \n",
    "#     else:\n",
    "    model_outputs = [column.test_outputs() for column in columns]      \n",
    "    position_ds   = 2\n",
    "    # average 10 outputs\n",
    "    avg_output = numpy.mean(model_outputs, axis=0)\n",
    "    # argmax over them\n",
    "    predictions = numpy.argmax(avg_output, axis=1)\n",
    "    # compare predictions with true labels\n",
    "    pred = T.ivector('pred')\n",
    "\n",
    "    all_true_labels_length = theano.function([], list(all_datasets.values())[0][position_ds][1].shape)\n",
    "    remainder = all_true_labels_length() - len(predictions)\n",
    "    true_labels = list(all_datasets.values())[0][position_ds][1][:]\n",
    "\n",
    "    error = theano.function([pred], T.mean(T.neq(pred, true_labels)))\n",
    "    acc = error(predictions.astype(dtype=numpy.int32))\n",
    "    print('....')\n",
    "    print('Error across %i columns: %f %%' % (len(models), 100*acc))\n",
    "    return [predictions, acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "streaming-value",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Starting to test 7 columns\n",
      "['/Users/models/mcdnn_nm20_trail0_Layers_time_1631081119.pkl', '/Users/models/mcdnn_nm0_trail0_Layers_time_1631066744.pkl', '/Users/models/mcdnn_nm10_trail0_Layers_time_1631069810.pkl', '/Users/models/mcdnn_nm12_trail0_Layers_time_1631072234.pkl', '/Users/models/mcdnn_nm16_trail0_Layers_time_1631077284.pkl', '/Users/models/mcdnn_nm18_trail0_Layers_time_1631079209.pkl', '/Users/models/mcdnn_nm14_trail0_Layers_time_1631074611.pkl']\n",
      "... loading data\n",
      "... normalizing digits to width 20 with extra padding 1\n",
      "... sharing data\n",
      "... building the dnn column\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-68-5cba53fdea8d>:39: UserWarning: DEPRECATION: the 'ds' parameter is not going to exist anymore as it is going to be replaced by the parameter 'ws'.\n",
      "  pooled_out = pool.pool_2d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading data\n",
      "... (un)padding digits from 28 -> 29\n",
      "... sharing data\n",
      "... building the dnn column\n",
      "... loading data\n",
      "... normalizing digits to width 10 with extra padding 1\n",
      "... sharing data\n",
      "... building the dnn column\n",
      "... loading data\n",
      "... normalizing digits to width 12 with extra padding 1\n",
      "... sharing data\n",
      "... building the dnn column\n",
      "... loading data\n",
      "... normalizing digits to width 16 with extra padding 1\n",
      "... sharing data\n",
      "... building the dnn column\n",
      "... loading data\n",
      "... normalizing digits to width 18 with extra padding 1\n",
      "... sharing data\n",
      "... building the dnn column\n",
      "... loading data\n",
      "... normalizing digits to width 14 with extra padding 1\n",
      "... sharing data\n",
      "... building the dnn column\n",
      "... Forward propagating 7 columns\n",
      "....\n",
      "Error across 7 columns: 4.650000 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([7, 2, 1, ..., 3, 9, 5]), array(0.0465)]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_columns()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-herald",
   "metadata": {},
   "source": [
    "Below is the exact same implementation as above but with dropout and the results I got for dropout are inconclusive maybe coz of the reduced training data. I got a greater test score which is 8.85%, this could be because of the missing information due to dropping of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "suited-allocation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... train 0 column of normalization 0\n",
      "... num_epochs 50\n",
      "... loading data\n",
      "... (un)padding digits from 28 -> 29\n",
      "... sharing data\n",
      "... building the dnn column\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pragnamandadi/opt/anaconda3/lib/python3.8/site-packages/theano/tensor/nnet/conv.py:98: UserWarning: theano.tensor.nnet.conv.conv2d is deprecated. Use theano.tensor.nnet.conv2d instead.\n",
      "  warnings.warn(\"theano.tensor.nnet.conv.conv2d is deprecated.\"\n",
      "<ipython-input-68-5cba53fdea8d>:39: UserWarning: DEPRECATION: the 'ds' parameter is not going to exist anymore as it is going to be replaced by the parameter 'ws'.\n",
      "  pooled_out = pool.pool_2d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... training\n",
      "training @ iter =  0\n",
      "epoch 1, minibatch 100/100, validation error 56.850000 %\n",
      "training @ iter =  100\n",
      "epoch 2, minibatch 100/100, validation error 28.250000 %\n",
      "training @ iter =  200\n",
      "epoch 3, minibatch 100/100, validation error 14.600000 %\n",
      "training @ iter =  300\n",
      "epoch 4, minibatch 100/100, validation error 11.200000 %\n",
      "training @ iter =  400\n",
      "epoch 5, minibatch 100/100, validation error 8.650000 %\n",
      "training @ iter =  500\n",
      "epoch 6, minibatch 100/100, validation error 7.150000 %\n",
      "training @ iter =  600\n",
      "epoch 7, minibatch 100/100, validation error 6.500000 %\n",
      "training @ iter =  700\n",
      "epoch 8, minibatch 100/100, validation error 5.650000 %\n",
      "training @ iter =  800\n",
      "epoch 9, minibatch 100/100, validation error 5.200000 %\n",
      "training @ iter =  900\n",
      "epoch 10, minibatch 100/100, validation error 5.400000 %\n",
      "training @ iter =  1000\n",
      "epoch 11, minibatch 100/100, validation error 5.350000 %\n",
      "training @ iter =  1100\n",
      "epoch 12, minibatch 100/100, validation error 4.600000 %\n",
      "training @ iter =  1200\n",
      "epoch 13, minibatch 100/100, validation error 3.950000 %\n",
      "training @ iter =  1300\n",
      "epoch 14, minibatch 100/100, validation error 4.250000 %\n",
      "training @ iter =  1400\n",
      "epoch 15, minibatch 100/100, validation error 4.000000 %\n",
      "training @ iter =  1500\n",
      "epoch 16, minibatch 100/100, validation error 3.600000 %\n",
      "training @ iter =  1600\n",
      "epoch 17, minibatch 100/100, validation error 3.550000 %\n",
      "training @ iter =  1700\n",
      "epoch 18, minibatch 100/100, validation error 3.650000 %\n",
      "training @ iter =  1800\n",
      "epoch 19, minibatch 100/100, validation error 3.650000 %\n",
      "training @ iter =  1900\n",
      "epoch 20, minibatch 100/100, validation error 3.100000 %\n",
      "training @ iter =  2000\n",
      "epoch 21, minibatch 100/100, validation error 3.350000 %\n",
      "training @ iter =  2100\n",
      "epoch 22, minibatch 100/100, validation error 3.200000 %\n",
      "training @ iter =  2200\n",
      "epoch 23, minibatch 100/100, validation error 3.450000 %\n",
      "training @ iter =  2300\n",
      "epoch 24, minibatch 100/100, validation error 3.450000 %\n",
      "training @ iter =  2400\n",
      "epoch 25, minibatch 100/100, validation error 2.700000 %\n",
      "training @ iter =  2500\n",
      "epoch 26, minibatch 100/100, validation error 2.650000 %\n",
      "training @ iter =  2600\n",
      "epoch 27, minibatch 100/100, validation error 2.950000 %\n",
      "training @ iter =  2700\n",
      "epoch 28, minibatch 100/100, validation error 3.250000 %\n",
      "training @ iter =  2800\n",
      "epoch 29, minibatch 100/100, validation error 2.900000 %\n",
      "training @ iter =  2900\n",
      "epoch 30, minibatch 100/100, validation error 3.050000 %\n",
      "training @ iter =  3000\n",
      "epoch 31, minibatch 100/100, validation error 3.350000 %\n",
      "training @ iter =  3100\n",
      "epoch 32, minibatch 100/100, validation error 3.150000 %\n",
      "training @ iter =  3200\n",
      "epoch 33, minibatch 100/100, validation error 2.900000 %\n",
      "training @ iter =  3300\n",
      "epoch 34, minibatch 100/100, validation error 3.000000 %\n",
      "training @ iter =  3400\n",
      "epoch 35, minibatch 100/100, validation error 3.050000 %\n",
      "training @ iter =  3500\n",
      "epoch 36, minibatch 100/100, validation error 2.700000 %\n",
      "training @ iter =  3600\n",
      "epoch 37, minibatch 100/100, validation error 2.900000 %\n",
      "training @ iter =  3700\n",
      "epoch 38, minibatch 100/100, validation error 2.400000 %\n",
      "training @ iter =  3800\n",
      "epoch 39, minibatch 100/100, validation error 2.700000 %\n",
      "training @ iter =  3900\n",
      "epoch 40, minibatch 100/100, validation error 2.800000 %\n",
      "training @ iter =  4000\n",
      "epoch 41, minibatch 100/100, validation error 2.750000 %\n",
      "training @ iter =  4100\n",
      "epoch 42, minibatch 100/100, validation error 2.400000 %\n",
      "training @ iter =  4200\n",
      "epoch 43, minibatch 100/100, validation error 2.450000 %\n",
      "training @ iter =  4300\n",
      "epoch 44, minibatch 100/100, validation error 2.600000 %\n",
      "training @ iter =  4400\n",
      "epoch 45, minibatch 100/100, validation error 2.750000 %\n",
      "training @ iter =  4500\n",
      "epoch 46, minibatch 100/100, validation error 2.650000 %\n",
      "training @ iter =  4600\n",
      "epoch 47, minibatch 100/100, validation error 3.300000 %\n",
      "training @ iter =  4700\n",
      "epoch 48, minibatch 100/100, validation error 2.300000 %\n",
      "training @ iter =  4800\n",
      "epoch 49, minibatch 100/100, validation error 3.000000 %\n",
      "training @ iter =  4900\n",
      "epoch 50, minibatch 100/100, validation error 3.000000 %\n",
      "Optimization complete.\n",
      "Best validation score of 2.300000 % obtained at iteration 4800, with test performance 0.000000 %\n",
      "<ipykernel.iostream.OutStream object at 0x7ffed0646790> The code for file pragnamandadi ran for 32.95m\n",
      "Saving Model as \"mcdnn_nm0_trail0_Layers_time_1631084788\"...\n",
      "... train 0 column of normalization 10\n",
      "... num_epochs 50\n",
      "... loading data\n",
      "... normalizing digits to width 10 with extra padding 1\n",
      "... sharing data\n",
      "... building the dnn column\n",
      "... training\n",
      "training @ iter =  0\n",
      "epoch 1, minibatch 100/100, validation error 79.400000 %\n",
      "training @ iter =  100\n",
      "epoch 2, minibatch 100/100, validation error 79.850000 %\n",
      "training @ iter =  200\n",
      "epoch 3, minibatch 100/100, validation error 80.000000 %\n",
      "training @ iter =  300\n",
      "epoch 4, minibatch 100/100, validation error 79.500000 %\n",
      "training @ iter =  400\n",
      "epoch 5, minibatch 100/100, validation error 80.050000 %\n",
      "training @ iter =  500\n",
      "epoch 6, minibatch 100/100, validation error 79.700000 %\n",
      "training @ iter =  600\n",
      "epoch 7, minibatch 100/100, validation error 79.600000 %\n",
      "training @ iter =  700\n",
      "epoch 8, minibatch 100/100, validation error 80.450000 %\n",
      "training @ iter =  800\n",
      "epoch 9, minibatch 100/100, validation error 81.400000 %\n",
      "training @ iter =  900\n",
      "epoch 10, minibatch 100/100, validation error 80.350000 %\n",
      "training @ iter =  1000\n",
      "epoch 11, minibatch 100/100, validation error 79.250000 %\n",
      "training @ iter =  1100\n",
      "epoch 12, minibatch 100/100, validation error 78.900000 %\n",
      "training @ iter =  1200\n",
      "epoch 13, minibatch 100/100, validation error 78.300000 %\n",
      "training @ iter =  1300\n",
      "epoch 14, minibatch 100/100, validation error 80.200000 %\n",
      "training @ iter =  1400\n",
      "epoch 15, minibatch 100/100, validation error 78.700000 %\n",
      "training @ iter =  1500\n",
      "epoch 16, minibatch 100/100, validation error 79.250000 %\n",
      "training @ iter =  1600\n",
      "epoch 17, minibatch 100/100, validation error 79.800000 %\n",
      "training @ iter =  1700\n",
      "epoch 18, minibatch 100/100, validation error 78.550000 %\n",
      "training @ iter =  1800\n",
      "epoch 19, minibatch 100/100, validation error 78.000000 %\n",
      "training @ iter =  1900\n",
      "epoch 20, minibatch 100/100, validation error 78.150000 %\n",
      "training @ iter =  2000\n",
      "epoch 21, minibatch 100/100, validation error 78.150000 %\n",
      "training @ iter =  2100\n",
      "epoch 22, minibatch 100/100, validation error 78.800000 %\n",
      "training @ iter =  2200\n",
      "epoch 23, minibatch 100/100, validation error 79.400000 %\n",
      "training @ iter =  2300\n",
      "epoch 24, minibatch 100/100, validation error 79.700000 %\n",
      "training @ iter =  2400\n",
      "epoch 25, minibatch 100/100, validation error 77.250000 %\n",
      "training @ iter =  2500\n",
      "epoch 26, minibatch 100/100, validation error 78.600000 %\n",
      "training @ iter =  2600\n",
      "epoch 27, minibatch 100/100, validation error 80.200000 %\n",
      "training @ iter =  2700\n",
      "epoch 28, minibatch 100/100, validation error 79.000000 %\n",
      "training @ iter =  2800\n",
      "epoch 29, minibatch 100/100, validation error 78.750000 %\n",
      "training @ iter =  2900\n",
      "epoch 30, minibatch 100/100, validation error 79.050000 %\n",
      "training @ iter =  3000\n",
      "epoch 31, minibatch 100/100, validation error 78.650000 %\n",
      "training @ iter =  3100\n",
      "epoch 32, minibatch 100/100, validation error 79.200000 %\n",
      "training @ iter =  3200\n",
      "epoch 33, minibatch 100/100, validation error 77.500000 %\n",
      "training @ iter =  3300\n",
      "epoch 34, minibatch 100/100, validation error 77.200000 %\n",
      "training @ iter =  3400\n",
      "epoch 35, minibatch 100/100, validation error 78.600000 %\n",
      "training @ iter =  3500\n",
      "epoch 36, minibatch 100/100, validation error 78.300000 %\n",
      "training @ iter =  3600\n",
      "epoch 37, minibatch 100/100, validation error 77.700000 %\n",
      "training @ iter =  3700\n",
      "epoch 38, minibatch 100/100, validation error 78.250000 %\n",
      "training @ iter =  3800\n",
      "epoch 39, minibatch 100/100, validation error 79.700000 %\n",
      "training @ iter =  3900\n",
      "epoch 40, minibatch 100/100, validation error 79.100000 %\n",
      "training @ iter =  4000\n",
      "epoch 41, minibatch 100/100, validation error 78.250000 %\n",
      "training @ iter =  4100\n",
      "epoch 42, minibatch 100/100, validation error 78.300000 %\n",
      "training @ iter =  4200\n",
      "epoch 43, minibatch 100/100, validation error 78.250000 %\n",
      "training @ iter =  4300\n",
      "epoch 44, minibatch 100/100, validation error 78.150000 %\n",
      "training @ iter =  4400\n",
      "epoch 45, minibatch 100/100, validation error 77.550000 %\n",
      "training @ iter =  4500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 46, minibatch 100/100, validation error 78.250000 %\n",
      "training @ iter =  4600\n",
      "epoch 47, minibatch 100/100, validation error 78.550000 %\n",
      "training @ iter =  4700\n",
      "epoch 48, minibatch 100/100, validation error 78.150000 %\n",
      "training @ iter =  4800\n",
      "epoch 49, minibatch 100/100, validation error 78.700000 %\n",
      "training @ iter =  4900\n",
      "epoch 50, minibatch 100/100, validation error 77.550000 %\n",
      "Optimization complete.\n",
      "Best validation score of 77.200000 % obtained at iteration 3400, with test performance 0.000000 %\n",
      "<ipykernel.iostream.OutStream object at 0x7ffed0646790> The code for file pragnamandadi ran for 33.01m\n",
      "Saving Model as \"mcdnn_nm10_trail0_Layers_time_1631086774\"...\n",
      "... train 0 column of normalization 12\n",
      "... num_epochs 50\n",
      "... loading data\n",
      "... normalizing digits to width 12 with extra padding 1\n",
      "... sharing data\n",
      "... building the dnn column\n",
      "... training\n",
      "training @ iter =  0\n",
      "epoch 1, minibatch 100/100, validation error 79.800000 %\n",
      "training @ iter =  100\n",
      "epoch 2, minibatch 100/100, validation error 80.300000 %\n",
      "training @ iter =  200\n",
      "epoch 3, minibatch 100/100, validation error 79.200000 %\n",
      "training @ iter =  300\n",
      "epoch 4, minibatch 100/100, validation error 76.850000 %\n",
      "training @ iter =  400\n",
      "epoch 5, minibatch 100/100, validation error 77.550000 %\n",
      "training @ iter =  500\n",
      "epoch 6, minibatch 100/100, validation error 76.500000 %\n",
      "training @ iter =  600\n",
      "epoch 7, minibatch 100/100, validation error 76.700000 %\n",
      "training @ iter =  700\n",
      "epoch 8, minibatch 100/100, validation error 78.050000 %\n",
      "training @ iter =  800\n",
      "epoch 9, minibatch 100/100, validation error 77.800000 %\n",
      "training @ iter =  900\n",
      "epoch 10, minibatch 100/100, validation error 77.250000 %\n",
      "training @ iter =  1000\n",
      "epoch 11, minibatch 100/100, validation error 76.950000 %\n",
      "training @ iter =  1100\n",
      "epoch 12, minibatch 100/100, validation error 77.200000 %\n",
      "training @ iter =  1200\n",
      "epoch 13, minibatch 100/100, validation error 76.500000 %\n",
      "training @ iter =  1300\n",
      "epoch 14, minibatch 100/100, validation error 77.000000 %\n",
      "training @ iter =  1400\n",
      "epoch 15, minibatch 100/100, validation error 75.700000 %\n",
      "training @ iter =  1500\n",
      "epoch 16, minibatch 100/100, validation error 75.800000 %\n",
      "training @ iter =  1600\n",
      "epoch 17, minibatch 100/100, validation error 74.650000 %\n",
      "training @ iter =  1700\n",
      "epoch 18, minibatch 100/100, validation error 74.450000 %\n",
      "training @ iter =  1800\n",
      "epoch 19, minibatch 100/100, validation error 73.150000 %\n",
      "training @ iter =  1900\n",
      "epoch 20, minibatch 100/100, validation error 74.650000 %\n",
      "training @ iter =  2000\n",
      "epoch 21, minibatch 100/100, validation error 73.500000 %\n",
      "training @ iter =  2100\n",
      "epoch 22, minibatch 100/100, validation error 74.250000 %\n",
      "training @ iter =  2200\n",
      "epoch 23, minibatch 100/100, validation error 74.300000 %\n",
      "training @ iter =  2300\n",
      "epoch 24, minibatch 100/100, validation error 73.950000 %\n",
      "training @ iter =  2400\n",
      "epoch 25, minibatch 100/100, validation error 72.700000 %\n",
      "training @ iter =  2500\n",
      "epoch 26, minibatch 100/100, validation error 73.250000 %\n",
      "training @ iter =  2600\n",
      "epoch 27, minibatch 100/100, validation error 74.250000 %\n",
      "training @ iter =  2700\n",
      "epoch 28, minibatch 100/100, validation error 73.850000 %\n",
      "training @ iter =  2800\n",
      "epoch 29, minibatch 100/100, validation error 73.400000 %\n",
      "training @ iter =  2900\n",
      "epoch 30, minibatch 100/100, validation error 72.550000 %\n",
      "training @ iter =  3000\n",
      "epoch 31, minibatch 100/100, validation error 73.400000 %\n",
      "training @ iter =  3100\n",
      "epoch 32, minibatch 100/100, validation error 73.100000 %\n",
      "training @ iter =  3200\n",
      "epoch 33, minibatch 100/100, validation error 72.000000 %\n",
      "training @ iter =  3300\n",
      "epoch 34, minibatch 100/100, validation error 71.550000 %\n",
      "training @ iter =  3400\n",
      "epoch 35, minibatch 100/100, validation error 73.000000 %\n",
      "training @ iter =  3500\n",
      "epoch 36, minibatch 100/100, validation error 71.750000 %\n",
      "training @ iter =  3600\n",
      "epoch 37, minibatch 100/100, validation error 71.500000 %\n",
      "training @ iter =  3700\n",
      "epoch 38, minibatch 100/100, validation error 72.100000 %\n",
      "training @ iter =  3800\n",
      "epoch 39, minibatch 100/100, validation error 71.100000 %\n",
      "training @ iter =  3900\n",
      "epoch 40, minibatch 100/100, validation error 70.550000 %\n",
      "training @ iter =  4000\n",
      "epoch 41, minibatch 100/100, validation error 70.950000 %\n",
      "training @ iter =  4100\n",
      "epoch 42, minibatch 100/100, validation error 70.150000 %\n",
      "training @ iter =  4200\n",
      "epoch 43, minibatch 100/100, validation error 68.000000 %\n",
      "training @ iter =  4300\n",
      "epoch 44, minibatch 100/100, validation error 67.550000 %\n",
      "training @ iter =  4400\n",
      "epoch 45, minibatch 100/100, validation error 66.450000 %\n",
      "training @ iter =  4500\n",
      "epoch 46, minibatch 100/100, validation error 67.500000 %\n",
      "training @ iter =  4600\n",
      "epoch 47, minibatch 100/100, validation error 67.350000 %\n",
      "training @ iter =  4700\n",
      "epoch 48, minibatch 100/100, validation error 65.350000 %\n",
      "training @ iter =  4800\n",
      "epoch 49, minibatch 100/100, validation error 64.300000 %\n",
      "training @ iter =  4900\n",
      "epoch 50, minibatch 100/100, validation error 65.350000 %\n",
      "Optimization complete.\n",
      "Best validation score of 64.300000 % obtained at iteration 4900, with test performance 0.000000 %\n",
      "<ipykernel.iostream.OutStream object at 0x7ffed0646790> The code for file pragnamandadi ran for 32.68m\n",
      "Saving Model as \"mcdnn_nm12_trail0_Layers_time_1631088740\"...\n",
      "... train 0 column of normalization 14\n",
      "... num_epochs 50\n",
      "... loading data\n",
      "... normalizing digits to width 14 with extra padding 1\n",
      "... sharing data\n",
      "... building the dnn column\n",
      "... training\n",
      "training @ iter =  0\n",
      "epoch 1, minibatch 100/100, validation error 79.450000 %\n",
      "training @ iter =  100\n",
      "epoch 2, minibatch 100/100, validation error 80.800000 %\n",
      "training @ iter =  200\n",
      "epoch 3, minibatch 100/100, validation error 77.300000 %\n",
      "training @ iter =  300\n",
      "epoch 4, minibatch 100/100, validation error 76.600000 %\n",
      "training @ iter =  400\n",
      "epoch 5, minibatch 100/100, validation error 75.350000 %\n",
      "training @ iter =  500\n",
      "epoch 6, minibatch 100/100, validation error 74.550000 %\n",
      "training @ iter =  600\n",
      "epoch 7, minibatch 100/100, validation error 72.150000 %\n",
      "training @ iter =  700\n",
      "epoch 8, minibatch 100/100, validation error 73.450000 %\n",
      "training @ iter =  800\n",
      "epoch 9, minibatch 100/100, validation error 72.100000 %\n",
      "training @ iter =  900\n",
      "epoch 10, minibatch 100/100, validation error 71.800000 %\n",
      "training @ iter =  1000\n",
      "epoch 11, minibatch 100/100, validation error 70.600000 %\n",
      "training @ iter =  1100\n",
      "epoch 12, minibatch 100/100, validation error 71.250000 %\n",
      "training @ iter =  1200\n",
      "epoch 13, minibatch 100/100, validation error 70.850000 %\n",
      "training @ iter =  1300\n",
      "epoch 14, minibatch 100/100, validation error 70.100000 %\n",
      "training @ iter =  1400\n",
      "epoch 15, minibatch 100/100, validation error 68.950000 %\n",
      "training @ iter =  1500\n",
      "epoch 16, minibatch 100/100, validation error 68.000000 %\n",
      "training @ iter =  1600\n",
      "epoch 17, minibatch 100/100, validation error 67.900000 %\n",
      "training @ iter =  1700\n",
      "epoch 18, minibatch 100/100, validation error 67.850000 %\n",
      "training @ iter =  1800\n",
      "epoch 19, minibatch 100/100, validation error 67.600000 %\n",
      "training @ iter =  1900\n",
      "epoch 20, minibatch 100/100, validation error 67.700000 %\n",
      "training @ iter =  2000\n",
      "epoch 21, minibatch 100/100, validation error 67.550000 %\n",
      "training @ iter =  2100\n",
      "epoch 22, minibatch 100/100, validation error 66.850000 %\n",
      "training @ iter =  2200\n",
      "epoch 23, minibatch 100/100, validation error 65.700000 %\n",
      "training @ iter =  2300\n",
      "epoch 24, minibatch 100/100, validation error 65.550000 %\n",
      "training @ iter =  2400\n",
      "epoch 25, minibatch 100/100, validation error 64.550000 %\n",
      "training @ iter =  2500\n",
      "epoch 26, minibatch 100/100, validation error 64.250000 %\n",
      "training @ iter =  2600\n",
      "epoch 27, minibatch 100/100, validation error 65.100000 %\n",
      "training @ iter =  2700\n",
      "epoch 28, minibatch 100/100, validation error 63.550000 %\n",
      "training @ iter =  2800\n",
      "epoch 29, minibatch 100/100, validation error 61.750000 %\n",
      "training @ iter =  2900\n",
      "epoch 30, minibatch 100/100, validation error 59.400000 %\n",
      "training @ iter =  3000\n",
      "epoch 31, minibatch 100/100, validation error 60.200000 %\n",
      "training @ iter =  3100\n",
      "epoch 32, minibatch 100/100, validation error 57.000000 %\n",
      "training @ iter =  3200\n",
      "epoch 33, minibatch 100/100, validation error 57.750000 %\n",
      "training @ iter =  3300\n",
      "epoch 34, minibatch 100/100, validation error 55.200000 %\n",
      "training @ iter =  3400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 35, minibatch 100/100, validation error 47.100000 %\n",
      "training @ iter =  3500\n",
      "epoch 36, minibatch 100/100, validation error 41.850000 %\n",
      "training @ iter =  3600\n",
      "epoch 37, minibatch 100/100, validation error 44.750000 %\n",
      "training @ iter =  3700\n",
      "epoch 38, minibatch 100/100, validation error 47.100000 %\n",
      "training @ iter =  3800\n",
      "epoch 39, minibatch 100/100, validation error 35.500000 %\n",
      "training @ iter =  3900\n",
      "epoch 40, minibatch 100/100, validation error 36.350000 %\n",
      "training @ iter =  4000\n",
      "epoch 41, minibatch 100/100, validation error 33.500000 %\n",
      "training @ iter =  4100\n",
      "epoch 42, minibatch 100/100, validation error 28.900000 %\n",
      "training @ iter =  4200\n",
      "epoch 43, minibatch 100/100, validation error 34.400000 %\n",
      "training @ iter =  4300\n",
      "epoch 44, minibatch 100/100, validation error 24.050000 %\n",
      "training @ iter =  4400\n",
      "epoch 45, minibatch 100/100, validation error 24.100000 %\n",
      "training @ iter =  4500\n",
      "epoch 46, minibatch 100/100, validation error 23.150000 %\n",
      "training @ iter =  4600\n",
      "epoch 47, minibatch 100/100, validation error 22.700000 %\n",
      "training @ iter =  4700\n",
      "epoch 48, minibatch 100/100, validation error 20.600000 %\n",
      "training @ iter =  4800\n",
      "epoch 49, minibatch 100/100, validation error 20.200000 %\n",
      "training @ iter =  4900\n",
      "epoch 50, minibatch 100/100, validation error 22.250000 %\n",
      "Optimization complete.\n",
      "Best validation score of 20.200000 % obtained at iteration 4900, with test performance 0.000000 %\n",
      "<ipykernel.iostream.OutStream object at 0x7ffed0646790> The code for file pragnamandadi ran for 30.90m\n",
      "Saving Model as \"mcdnn_nm14_trail0_Layers_time_1631090599\"...\n",
      "... train 0 column of normalization 16\n",
      "... num_epochs 50\n",
      "... loading data\n",
      "... normalizing digits to width 16 with extra padding 1\n",
      "... sharing data\n",
      "... building the dnn column\n",
      "... training\n",
      "training @ iter =  0\n",
      "epoch 1, minibatch 100/100, validation error 81.750000 %\n",
      "training @ iter =  100\n",
      "epoch 2, minibatch 100/100, validation error 80.100000 %\n",
      "training @ iter =  200\n",
      "epoch 3, minibatch 100/100, validation error 74.850000 %\n",
      "training @ iter =  300\n",
      "epoch 4, minibatch 100/100, validation error 74.700000 %\n",
      "training @ iter =  400\n",
      "epoch 5, minibatch 100/100, validation error 73.550000 %\n",
      "training @ iter =  500\n",
      "epoch 6, minibatch 100/100, validation error 71.900000 %\n",
      "training @ iter =  600\n",
      "epoch 7, minibatch 100/100, validation error 70.750000 %\n",
      "training @ iter =  700\n",
      "epoch 8, minibatch 100/100, validation error 70.050000 %\n",
      "training @ iter =  800\n",
      "epoch 9, minibatch 100/100, validation error 69.100000 %\n",
      "training @ iter =  900\n",
      "epoch 10, minibatch 100/100, validation error 69.750000 %\n",
      "training @ iter =  1000\n",
      "epoch 11, minibatch 100/100, validation error 68.450000 %\n",
      "training @ iter =  1100\n",
      "epoch 12, minibatch 100/100, validation error 68.450000 %\n",
      "training @ iter =  1200\n",
      "epoch 13, minibatch 100/100, validation error 69.600000 %\n",
      "training @ iter =  1300\n",
      "epoch 14, minibatch 100/100, validation error 67.550000 %\n",
      "training @ iter =  1400\n",
      "epoch 15, minibatch 100/100, validation error 67.350000 %\n",
      "training @ iter =  1500\n",
      "epoch 16, minibatch 100/100, validation error 65.900000 %\n",
      "training @ iter =  1600\n",
      "epoch 17, minibatch 100/100, validation error 65.000000 %\n",
      "training @ iter =  1700\n",
      "epoch 18, minibatch 100/100, validation error 64.650000 %\n",
      "training @ iter =  1800\n",
      "epoch 19, minibatch 100/100, validation error 63.250000 %\n",
      "training @ iter =  1900\n",
      "epoch 20, minibatch 100/100, validation error 62.050000 %\n",
      "training @ iter =  2000\n",
      "epoch 21, minibatch 100/100, validation error 61.600000 %\n",
      "training @ iter =  2100\n",
      "epoch 22, minibatch 100/100, validation error 60.900000 %\n",
      "training @ iter =  2200\n",
      "epoch 23, minibatch 100/100, validation error 56.500000 %\n",
      "training @ iter =  2300\n",
      "epoch 24, minibatch 100/100, validation error 54.100000 %\n",
      "training @ iter =  2400\n",
      "epoch 25, minibatch 100/100, validation error 54.450000 %\n",
      "training @ iter =  2500\n",
      "epoch 26, minibatch 100/100, validation error 51.750000 %\n",
      "training @ iter =  2600\n",
      "epoch 27, minibatch 100/100, validation error 52.700000 %\n",
      "training @ iter =  2700\n",
      "epoch 28, minibatch 100/100, validation error 50.500000 %\n",
      "training @ iter =  2800\n",
      "epoch 29, minibatch 100/100, validation error 44.550000 %\n",
      "training @ iter =  2900\n",
      "epoch 30, minibatch 100/100, validation error 40.500000 %\n",
      "training @ iter =  3000\n",
      "epoch 31, minibatch 100/100, validation error 33.500000 %\n",
      "training @ iter =  3100\n",
      "epoch 32, minibatch 100/100, validation error 35.400000 %\n",
      "training @ iter =  3200\n",
      "epoch 33, minibatch 100/100, validation error 31.450000 %\n",
      "training @ iter =  3300\n",
      "epoch 34, minibatch 100/100, validation error 28.950000 %\n",
      "training @ iter =  3400\n",
      "epoch 35, minibatch 100/100, validation error 26.500000 %\n",
      "training @ iter =  3500\n",
      "epoch 36, minibatch 100/100, validation error 24.450000 %\n",
      "training @ iter =  3600\n",
      "epoch 37, minibatch 100/100, validation error 24.600000 %\n",
      "training @ iter =  3700\n",
      "epoch 38, minibatch 100/100, validation error 21.600000 %\n",
      "training @ iter =  3800\n",
      "epoch 39, minibatch 100/100, validation error 20.250000 %\n",
      "training @ iter =  3900\n",
      "epoch 40, minibatch 100/100, validation error 19.050000 %\n",
      "training @ iter =  4000\n",
      "epoch 41, minibatch 100/100, validation error 18.350000 %\n",
      "training @ iter =  4100\n",
      "epoch 42, minibatch 100/100, validation error 18.300000 %\n",
      "training @ iter =  4200\n",
      "epoch 43, minibatch 100/100, validation error 17.050000 %\n",
      "training @ iter =  4300\n",
      "epoch 44, minibatch 100/100, validation error 14.550000 %\n",
      "training @ iter =  4400\n",
      "epoch 45, minibatch 100/100, validation error 16.900000 %\n",
      "training @ iter =  4500\n",
      "epoch 46, minibatch 100/100, validation error 17.650000 %\n",
      "training @ iter =  4600\n",
      "epoch 47, minibatch 100/100, validation error 15.600000 %\n",
      "training @ iter =  4700\n",
      "epoch 48, minibatch 100/100, validation error 13.900000 %\n",
      "training @ iter =  4800\n",
      "epoch 49, minibatch 100/100, validation error 15.000000 %\n",
      "training @ iter =  4900\n",
      "epoch 50, minibatch 100/100, validation error 13.550000 %\n",
      "Optimization complete.\n",
      "Best validation score of 13.550000 % obtained at iteration 5000, with test performance 0.000000 %\n",
      "<ipykernel.iostream.OutStream object at 0x7ffed0646790> The code for file pragnamandadi ran for 30.58m\n",
      "Saving Model as \"mcdnn_nm16_trail0_Layers_time_1631092438\"...\n",
      "... train 0 column of normalization 18\n",
      "... num_epochs 50\n",
      "... loading data\n",
      "... normalizing digits to width 18 with extra padding 1\n",
      "... sharing data\n",
      "... building the dnn column\n",
      "... training\n",
      "training @ iter =  0\n",
      "epoch 1, minibatch 100/100, validation error 80.350000 %\n",
      "training @ iter =  100\n",
      "epoch 2, minibatch 100/100, validation error 78.200000 %\n",
      "training @ iter =  200\n",
      "epoch 3, minibatch 100/100, validation error 77.650000 %\n",
      "training @ iter =  300\n",
      "epoch 4, minibatch 100/100, validation error 77.000000 %\n",
      "training @ iter =  400\n",
      "epoch 5, minibatch 100/100, validation error 76.300000 %\n",
      "training @ iter =  500\n",
      "epoch 6, minibatch 100/100, validation error 74.600000 %\n",
      "training @ iter =  600\n",
      "epoch 7, minibatch 100/100, validation error 72.850000 %\n",
      "training @ iter =  700\n",
      "epoch 8, minibatch 100/100, validation error 73.150000 %\n",
      "training @ iter =  800\n",
      "epoch 9, minibatch 100/100, validation error 72.050000 %\n",
      "training @ iter =  900\n",
      "epoch 10, minibatch 100/100, validation error 72.850000 %\n",
      "training @ iter =  1000\n",
      "epoch 11, minibatch 100/100, validation error 71.750000 %\n",
      "training @ iter =  1100\n",
      "epoch 12, minibatch 100/100, validation error 71.400000 %\n",
      "training @ iter =  1200\n",
      "epoch 13, minibatch 100/100, validation error 71.300000 %\n",
      "training @ iter =  1300\n",
      "epoch 14, minibatch 100/100, validation error 69.900000 %\n",
      "training @ iter =  1400\n",
      "epoch 15, minibatch 100/100, validation error 70.300000 %\n",
      "training @ iter =  1500\n",
      "epoch 16, minibatch 100/100, validation error 69.150000 %\n",
      "training @ iter =  1600\n",
      "epoch 17, minibatch 100/100, validation error 67.800000 %\n",
      "training @ iter =  1700\n",
      "epoch 18, minibatch 100/100, validation error 66.950000 %\n",
      "training @ iter =  1800\n",
      "epoch 19, minibatch 100/100, validation error 65.050000 %\n",
      "training @ iter =  1900\n",
      "epoch 20, minibatch 100/100, validation error 63.400000 %\n",
      "training @ iter =  2000\n",
      "epoch 21, minibatch 100/100, validation error 62.100000 %\n",
      "training @ iter =  2100\n",
      "epoch 22, minibatch 100/100, validation error 62.750000 %\n",
      "training @ iter =  2200\n",
      "epoch 23, minibatch 100/100, validation error 61.700000 %\n",
      "training @ iter =  2300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24, minibatch 100/100, validation error 56.850000 %\n",
      "training @ iter =  2400\n",
      "epoch 25, minibatch 100/100, validation error 60.450000 %\n",
      "training @ iter =  2500\n",
      "epoch 26, minibatch 100/100, validation error 54.050000 %\n",
      "training @ iter =  2600\n",
      "epoch 27, minibatch 100/100, validation error 55.800000 %\n",
      "training @ iter =  2700\n",
      "epoch 28, minibatch 100/100, validation error 48.050000 %\n",
      "training @ iter =  2800\n",
      "epoch 29, minibatch 100/100, validation error 49.100000 %\n",
      "training @ iter =  2900\n",
      "epoch 30, minibatch 100/100, validation error 39.450000 %\n",
      "training @ iter =  3000\n",
      "epoch 31, minibatch 100/100, validation error 38.450000 %\n",
      "training @ iter =  3100\n",
      "epoch 32, minibatch 100/100, validation error 33.600000 %\n",
      "training @ iter =  3200\n",
      "epoch 33, minibatch 100/100, validation error 31.800000 %\n",
      "training @ iter =  3300\n",
      "epoch 34, minibatch 100/100, validation error 33.650000 %\n",
      "training @ iter =  3400\n",
      "epoch 35, minibatch 100/100, validation error 29.750000 %\n",
      "training @ iter =  3500\n",
      "epoch 36, minibatch 100/100, validation error 27.750000 %\n",
      "training @ iter =  3600\n",
      "epoch 37, minibatch 100/100, validation error 29.050000 %\n",
      "training @ iter =  3700\n",
      "epoch 38, minibatch 100/100, validation error 21.000000 %\n",
      "training @ iter =  3800\n",
      "epoch 39, minibatch 100/100, validation error 21.400000 %\n",
      "training @ iter =  3900\n",
      "epoch 40, minibatch 100/100, validation error 20.350000 %\n",
      "training @ iter =  4000\n",
      "epoch 41, minibatch 100/100, validation error 18.550000 %\n",
      "training @ iter =  4100\n",
      "epoch 42, minibatch 100/100, validation error 20.100000 %\n",
      "training @ iter =  4200\n",
      "epoch 43, minibatch 100/100, validation error 18.300000 %\n",
      "training @ iter =  4300\n",
      "epoch 44, minibatch 100/100, validation error 17.600000 %\n",
      "training @ iter =  4400\n",
      "epoch 45, minibatch 100/100, validation error 17.300000 %\n",
      "training @ iter =  4500\n",
      "epoch 46, minibatch 100/100, validation error 17.650000 %\n",
      "training @ iter =  4600\n",
      "epoch 47, minibatch 100/100, validation error 15.700000 %\n",
      "training @ iter =  4700\n",
      "epoch 48, minibatch 100/100, validation error 15.400000 %\n",
      "training @ iter =  4800\n",
      "epoch 49, minibatch 100/100, validation error 14.800000 %\n",
      "training @ iter =  4900\n",
      "epoch 50, minibatch 100/100, validation error 14.950000 %\n",
      "Optimization complete.\n",
      "Best validation score of 14.800000 % obtained at iteration 4900, with test performance 0.000000 %\n",
      "<ipykernel.iostream.OutStream object at 0x7ffed0646790> The code for file pragnamandadi ran for 30.61m\n",
      "Saving Model as \"mcdnn_nm18_trail0_Layers_time_1631094280\"...\n",
      "... train 0 column of normalization 20\n",
      "... num_epochs 50\n",
      "... loading data\n",
      "... normalizing digits to width 20 with extra padding 1\n",
      "... sharing data\n",
      "... building the dnn column\n",
      "... training\n",
      "training @ iter =  0\n",
      "epoch 1, minibatch 100/100, validation error 79.200000 %\n",
      "training @ iter =  100\n",
      "epoch 2, minibatch 100/100, validation error 74.100000 %\n",
      "training @ iter =  200\n",
      "epoch 3, minibatch 100/100, validation error 69.850000 %\n",
      "training @ iter =  300\n",
      "epoch 4, minibatch 100/100, validation error 68.200000 %\n",
      "training @ iter =  400\n",
      "epoch 5, minibatch 100/100, validation error 66.100000 %\n",
      "training @ iter =  500\n",
      "epoch 6, minibatch 100/100, validation error 65.850000 %\n",
      "training @ iter =  600\n",
      "epoch 7, minibatch 100/100, validation error 64.100000 %\n",
      "training @ iter =  700\n",
      "epoch 8, minibatch 100/100, validation error 62.750000 %\n",
      "training @ iter =  800\n",
      "epoch 9, minibatch 100/100, validation error 63.600000 %\n",
      "training @ iter =  900\n",
      "epoch 10, minibatch 100/100, validation error 62.750000 %\n",
      "training @ iter =  1000\n",
      "epoch 11, minibatch 100/100, validation error 62.100000 %\n",
      "training @ iter =  1100\n",
      "epoch 12, minibatch 100/100, validation error 62.400000 %\n",
      "training @ iter =  1200\n",
      "epoch 13, minibatch 100/100, validation error 62.000000 %\n",
      "training @ iter =  1300\n",
      "epoch 14, minibatch 100/100, validation error 59.900000 %\n",
      "training @ iter =  1400\n",
      "epoch 15, minibatch 100/100, validation error 59.400000 %\n",
      "training @ iter =  1500\n",
      "epoch 16, minibatch 100/100, validation error 58.000000 %\n",
      "training @ iter =  1600\n",
      "epoch 17, minibatch 100/100, validation error 58.500000 %\n",
      "training @ iter =  1700\n",
      "epoch 18, minibatch 100/100, validation error 56.750000 %\n",
      "training @ iter =  1800\n",
      "epoch 19, minibatch 100/100, validation error 54.550000 %\n",
      "training @ iter =  1900\n",
      "epoch 20, minibatch 100/100, validation error 54.200000 %\n",
      "training @ iter =  2000\n",
      "epoch 21, minibatch 100/100, validation error 50.450000 %\n",
      "training @ iter =  2100\n",
      "epoch 22, minibatch 100/100, validation error 52.400000 %\n",
      "training @ iter =  2200\n",
      "epoch 23, minibatch 100/100, validation error 48.100000 %\n",
      "training @ iter =  2300\n",
      "epoch 24, minibatch 100/100, validation error 44.000000 %\n",
      "training @ iter =  2400\n",
      "epoch 25, minibatch 100/100, validation error 42.850000 %\n",
      "training @ iter =  2500\n",
      "epoch 26, minibatch 100/100, validation error 39.500000 %\n",
      "training @ iter =  2600\n",
      "epoch 27, minibatch 100/100, validation error 38.150000 %\n",
      "training @ iter =  2700\n",
      "epoch 28, minibatch 100/100, validation error 32.450000 %\n",
      "training @ iter =  2800\n",
      "epoch 29, minibatch 100/100, validation error 28.250000 %\n",
      "training @ iter =  2900\n",
      "epoch 30, minibatch 100/100, validation error 25.850000 %\n",
      "training @ iter =  3000\n",
      "epoch 31, minibatch 100/100, validation error 21.050000 %\n",
      "training @ iter =  3100\n",
      "epoch 32, minibatch 100/100, validation error 19.850000 %\n",
      "training @ iter =  3200\n",
      "epoch 33, minibatch 100/100, validation error 17.650000 %\n",
      "training @ iter =  3300\n",
      "epoch 34, minibatch 100/100, validation error 18.000000 %\n",
      "training @ iter =  3400\n",
      "epoch 35, minibatch 100/100, validation error 15.650000 %\n",
      "training @ iter =  3500\n",
      "epoch 36, minibatch 100/100, validation error 14.600000 %\n",
      "training @ iter =  3600\n",
      "epoch 37, minibatch 100/100, validation error 15.600000 %\n",
      "training @ iter =  3700\n",
      "epoch 38, minibatch 100/100, validation error 12.600000 %\n",
      "training @ iter =  3800\n",
      "epoch 39, minibatch 100/100, validation error 13.750000 %\n",
      "training @ iter =  3900\n",
      "epoch 40, minibatch 100/100, validation error 12.950000 %\n",
      "training @ iter =  4000\n",
      "epoch 41, minibatch 100/100, validation error 13.000000 %\n",
      "training @ iter =  4100\n",
      "epoch 42, minibatch 100/100, validation error 12.300000 %\n",
      "training @ iter =  4200\n",
      "epoch 43, minibatch 100/100, validation error 11.250000 %\n",
      "training @ iter =  4300\n",
      "epoch 44, minibatch 100/100, validation error 11.500000 %\n",
      "training @ iter =  4400\n",
      "epoch 45, minibatch 100/100, validation error 11.550000 %\n",
      "training @ iter =  4500\n",
      "epoch 46, minibatch 100/100, validation error 11.100000 %\n",
      "training @ iter =  4600\n",
      "epoch 47, minibatch 100/100, validation error 11.300000 %\n",
      "training @ iter =  4700\n",
      "epoch 48, minibatch 100/100, validation error 11.100000 %\n",
      "training @ iter =  4800\n",
      "epoch 49, minibatch 100/100, validation error 10.400000 %\n",
      "training @ iter =  4900\n",
      "epoch 50, minibatch 100/100, validation error 11.100000 %\n",
      "Optimization complete.\n",
      "Best validation score of 10.400000 % obtained at iteration 4900, with test performance 0.000000 %\n",
      "<ipykernel.iostream.OutStream object at 0x7ffed0646790> The code for file pragnamandadi ran for 30.61m\n",
      "Saving Model as \"mcdnn_nm20_trail0_Layers_time_1631096121\"...\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "for nm in [0,10,12,14,16,18,20]:\n",
    "    train_mcdnn_column(nm, n_epochs=50, trail=0, dropout = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "improved-weight",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import glob\n",
    "def test_dropout_columns():\n",
    "    dataset='mnist.pkl.gz'\n",
    "    # create data hash that will be filled with data from different normalizations\n",
    "    all_datasets = {}\n",
    "    # instantiate multiple columns\n",
    "    columns = []\n",
    "    models = files = glob.glob('/Users/Dropoutmodels/' + \"/*.pkl\")\n",
    "    print('... Starting to test %i columns' % len(models))\n",
    "    print(models)\n",
    "    for model in models:\n",
    "        # load model params\n",
    "        f = open(model,'rb')\n",
    "        u = pickle._Unpickler(f)\n",
    "        u.encoding = 'latin1'\n",
    "        params = u.load()\n",
    "        nkerns, batch_size, normalized_width, distortion = pickle.load(f)\n",
    "        if all_datasets.get(normalized_width):\n",
    "            datasets = all_datasets[normalized_width]\n",
    "        else:\n",
    "            datasets = load_data(dataset, normalized_width, 29)\n",
    "            all_datasets[normalized_width] = datasets\n",
    "        # no distortion during testing\n",
    "        columns.append(DNNColumn(datasets, nkerns, batch_size, normalized_width, 0, params))\n",
    "    print('... Forward propagating %i columns' % len(models))\n",
    "    # call test on all of them recieving 10 outputs\n",
    "#     if valid_test=='V':\n",
    "#         model_outputs = [column.valid_outputs() for column in columns] \n",
    "#         position_ds   = 1 \n",
    "#     else:\n",
    "    model_outputs = [column.test_outputs() for column in columns]      \n",
    "    position_ds   = 2\n",
    "    # average 10 outputs\n",
    "    avg_output = numpy.mean(model_outputs, axis=0)\n",
    "    # argmax over them\n",
    "    predictions = numpy.argmax(avg_output, axis=1)\n",
    "    # compare predictions with true labels\n",
    "    pred = T.ivector('pred')\n",
    "\n",
    "    all_true_labels_length = theano.function([], list(all_datasets.values())[0][position_ds][1].shape)\n",
    "    remainder = all_true_labels_length() - len(predictions)\n",
    "    true_labels = list(all_datasets.values())[0][position_ds][1][:]\n",
    "\n",
    "    error = theano.function([pred], T.mean(T.neq(pred, true_labels)))\n",
    "    acc = error(predictions.astype(dtype=numpy.int32))\n",
    "    print('....')\n",
    "    print('Error across %i columns: %f %%' % (len(models), 100*acc))\n",
    "    return [predictions, acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "handy-cornell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Starting to test 7 columns\n",
      "['/Users/Dropoutmodels/mcdnn_nm14_trail0_Layers_time_1631090599.pkl', '/Users/Dropoutmodels/mcdnn_nm16_trail0_Layers_time_1631092438.pkl', '/Users/Dropoutmodels/mcdnn_nm12_trail0_Layers_time_1631088740.pkl', '/Users/Dropoutmodels/mcdnn_nm0_trail0_Layers_time_1631084788.pkl', '/Users/Dropoutmodels/mcdnn_nm10_trail0_Layers_time_1631086774.pkl', '/Users/Dropoutmodels/mcdnn_nm18_trail0_Layers_time_1631094280.pkl', '/Users/Dropoutmodels/mcdnn_nm20_trail0_Layers_time_1631096121.pkl']\n",
      "... loading data\n",
      "... normalizing digits to width 14 with extra padding 1\n",
      "... sharing data\n",
      "... building the dnn column\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pragnamandadi/opt/anaconda3/lib/python3.8/site-packages/theano/tensor/nnet/conv.py:98: UserWarning: theano.tensor.nnet.conv.conv2d is deprecated. Use theano.tensor.nnet.conv2d instead.\n",
      "  warnings.warn(\"theano.tensor.nnet.conv.conv2d is deprecated.\"\n",
      "<ipython-input-68-5cba53fdea8d>:39: UserWarning: DEPRECATION: the 'ds' parameter is not going to exist anymore as it is going to be replaced by the parameter 'ws'.\n",
      "  pooled_out = pool.pool_2d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading data\n",
      "... normalizing digits to width 16 with extra padding 1\n",
      "... sharing data\n",
      "... building the dnn column\n",
      "... loading data\n",
      "... normalizing digits to width 12 with extra padding 1\n",
      "... sharing data\n",
      "... building the dnn column\n",
      "... loading data\n",
      "... (un)padding digits from 28 -> 29\n",
      "... sharing data\n",
      "... building the dnn column\n",
      "... loading data\n",
      "... normalizing digits to width 10 with extra padding 1\n",
      "... sharing data\n",
      "... building the dnn column\n",
      "... loading data\n",
      "... normalizing digits to width 18 with extra padding 1\n",
      "... sharing data\n",
      "... building the dnn column\n",
      "... loading data\n",
      "... normalizing digits to width 20 with extra padding 1\n",
      "... sharing data\n",
      "... building the dnn column\n",
      "... Forward propagating 7 columns\n",
      "....\n",
      "Error across 7 columns: 8.850000 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([7, 2, 1, ..., 3, 9, 5]), array(0.0885)]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dropout_columns()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-poetry",
   "metadata": {},
   "source": [
    "_Conclusion_\n",
    "In conclusion even though the claims are that the MCDNN performs better than a regular CNN, I wasn't able to achieve that due to the reduced amount of trainings per normalization and reduced epochs. But In my analysis if you are with reduced computational resources CNN still works better with 2.65% test_score which would have reduced to something below 1.5% if MCDNN was used but the trade off of the Computational resources is not worth it in my opinion if you are looking for cost effective model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
